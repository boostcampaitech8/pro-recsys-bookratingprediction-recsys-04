{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6a8c64",
   "metadata": {},
   "source": [
    "# ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ \n",
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0b2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "from typing import Tuple, Dict\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a4ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Languageë¥¼ ISBN ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class LanguageParserByISBN:\n",
    "    def __init__(self):\n",
    "        \"\"\"ISBN ì ‘ë‘ì‚¬ â†’ Language ë§¤í•‘\"\"\"\n",
    "        self.isbn_language_map = {\n",
    "            # ì˜ì–´ê¶Œ (0, 1)\n",
    "            \"0\": \"en\",\n",
    "            \"1\": \"en\",\n",
    "            # í”„ë‘ìŠ¤ì–´ê¶Œ (2)\n",
    "            \"2\": \"fr\",\n",
    "            # ë…ì¼ì–´ê¶Œ (3)\n",
    "            \"3\": \"de\",\n",
    "            # ì¼ë³¸ (4)\n",
    "            \"4\": \"ja\",\n",
    "            # ëŸ¬ì‹œì•„ (5)\n",
    "            \"5\": \"ru\",\n",
    "            # ì¤‘êµ­ (7)\n",
    "            \"7\": \"zh\",\n",
    "            # ì²´ì½”/ìŠ¬ë¡œë°”í‚¤ì•„ (80)\n",
    "            \"80\": \"cs\",\n",
    "            # ì¸ë„ (81)\n",
    "            \"81\": \"hi\",\n",
    "            # ë…¸ë¥´ì›¨ì´ (82)\n",
    "            \"82\": \"no\",\n",
    "            # í´ë€ë“œ (83)\n",
    "            \"83\": \"pl\",\n",
    "            # ìŠ¤í˜ì¸ (84)\n",
    "            \"84\": \"es\",\n",
    "            # ë¸Œë¼ì§ˆ (85)\n",
    "            \"85\": \"pt\",\n",
    "            # ìœ ê³ ìŠ¬ë¼ë¹„ì•„ (86)\n",
    "            \"86\": \"sr\",\n",
    "            # ë´ë§ˆí¬ (87)\n",
    "            \"87\": \"da\",\n",
    "            # ì´íƒˆë¦¬ì•„ (88)\n",
    "            \"88\": \"it\",\n",
    "            # í•œêµ­ (89)\n",
    "            \"89\": \"ko\",\n",
    "            # ë„¤ëœë€ë“œ (90)\n",
    "            \"90\": \"nl\",\n",
    "            # ìŠ¤ì›¨ë´ (91)\n",
    "            \"91\": \"sv\",\n",
    "            # í¬ë¥´íˆ¬ê°ˆ (972)\n",
    "            \"972\": \"pt\",\n",
    "            # í„°í‚¤ (975)\n",
    "            \"975\": \"tr\",\n",
    "            # ISBN-13 ë³€í™˜ ì ‘ë‘ì‚¬\n",
    "            \"978\": \"en\",\n",
    "            \"979\": \"en\",\n",
    "        }\n",
    "\n",
    "    def extract_isbn_prefix(self, isbn: str) -> str:\n",
    "        \"\"\"ISBNì—ì„œ Group Identifier ì¶”ì¶œ\"\"\"\n",
    "        if pd.isna(isbn):\n",
    "            return None\n",
    "\n",
    "        isbn_str = str(isbn).strip()\n",
    "\n",
    "        # ISBN-13ì¸ ê²½ìš° (978 ë˜ëŠ” 979ë¡œ ì‹œì‘)\n",
    "        if isbn_str.startswith((\"978\", \"979\")):\n",
    "            if len(isbn_str) >= 4:\n",
    "                return isbn_str[3]\n",
    "\n",
    "        # 3ìë¦¬ ì ‘ë‘ì‚¬ í™•ì¸ (ì˜ˆ: 972)\n",
    "        if len(isbn_str) >= 3 and isbn_str[:3] in self.isbn_language_map:\n",
    "            return isbn_str[:3]\n",
    "\n",
    "        # 2ìë¦¬ ì ‘ë‘ì‚¬ í™•ì¸ (ì˜ˆ: 80, 84)\n",
    "        if len(isbn_str) >= 2 and isbn_str[:2] in self.isbn_language_map:\n",
    "            return isbn_str[:2]\n",
    "\n",
    "        # 1ìë¦¬ ì ‘ë‘ì‚¬ (ì˜ˆ: 0, 1, 2, 3)\n",
    "        if len(isbn_str) >= 1 and isbn_str[0] in self.isbn_language_map:\n",
    "            return isbn_str[0]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def infer_language_from_isbn(self, isbn: str, default: str = \"en\") -> str:\n",
    "        \"\"\"ISBNìœ¼ë¡œë¶€í„° Language ì¶”ë¡ \"\"\"\n",
    "        prefix = self.extract_isbn_prefix(isbn)\n",
    "\n",
    "        if prefix is None:\n",
    "            return default\n",
    "\n",
    "        return self.isbn_language_map.get(prefix, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708c1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ì›ë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° >\n",
      "- Users: 68,092 rows\n",
      "- Books: 149,570 rows\n",
      "- Train: 306,795 rows\n",
      "- Test: 76,699 rows\n",
      "\n",
      "> User Age ì •ì œ ì‹œì‘\n",
      "\n",
      "  - ê²°ì¸¡ì¹˜ 27,833ê±´ â†’ ì¤‘ì•™ê°’(34ì„¸)ë¡œ ëŒ€ì²´\n",
      "> Age ì •ì œ ì™„ë£Œ (ì¤‘ì•™ê°’: 34ì„¸)\n",
      "\n",
      "> User Location ì •ì œ ì‹œì‘\n",
      "** Country: 586ê±´ ë³µì›\n",
      "** Country: ìµœë¹ˆê°’(usa)ìœ¼ë¡œ 932ê±´ ëŒ€ì²´\n",
      "** State: 1140ê±´ ë³µì›\n",
      "> Location ì •ì œ ì™„ë£Œ\n",
      "\n",
      "> Books ë°ì´í„° ì •ì œ ì‹œì‘\n",
      "** ì¶œíŒì—°ë„: ë¹„ì •ìƒ ë²”ìœ„ 3ê±´ â†’ ì¤‘ê°„ê°’(1996)ìœ¼ë¡œ ëŒ€ì²´\n",
      "** ì›ë³¸ ê²°ì¸¡ì¹˜: 67,227ê±´\n",
      "** ISBNìœ¼ë¡œ ìœ ì¶”í•œ ê°œìˆ˜: 67,227ê±´ (100.0%)\n",
      "\n",
      "  [ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸ (Top 5)]\n",
      "- en: 56,369ê±´\n",
      "- de: 5,284ê±´\n",
      "- fr: 2,420ê±´\n",
      "- es: 2,415ê±´\n",
      "- it: 355ê±´\n",
      "** Category: ê²°ì¸¡ì¹˜ 68,851ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\n",
      "** Book Author: ê²°ì¸¡ì¹˜ 1ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\n",
      "> Books ì •ì œ ì™„ë£Œ\n",
      "\n",
      "> Ratings ë°ì´í„° ì •ì œ ì‹œì‘\n",
      "> Ratings ì •ì œ ì™„ë£Œ\n",
      "\n",
      ">>> ì •ì œëœ ë°ì´í„° ì €ì¥ ì‹œì‘\n",
      "** ì €ì¥ ê²½ë¡œ: /data/ephemeral/home/sojin/data/v2/\n",
      "  - /data/ephemeral/home/sojin/data/v2/users.csv\n",
      "  - /data/ephemeral/home/sojin/data/v2/books.csv\n",
      "  - /data/ephemeral/home/sojin/data/v2/train_ratings.csv\n",
      "  - /data/ephemeral/home/sojin/data/v2/test_ratings.csv\n",
      "\n",
      "<<< ì €ì¥ ì™„ë£Œ! >>>\n",
      "\n",
      "==========\n",
      "                    >>> ë°ì´í„° ì •ì œ ê²°ê³¼ >>> \n",
      "==========\n",
      "\n",
      "[ì›ë³¸ ë°ì´í„° í¬ê¸°]\n",
      "  - Users: 68,092 rows\n",
      "  - Books: 149,570 rows\n",
      "  - Train: 306,795 rows\n",
      "  - Test: 76,699 rows\n",
      "\n",
      "[Age ì •ì œ]\n",
      "- ê²°ì¸¡ì¹˜ ëŒ€ì²´: 27,833ê±´\n",
      "\n",
      "[Location ì •ì œ]\n",
      "- Country ë³µì›: 586ê±´\n",
      "- State ë³µì›: 1140ê±´\n",
      "- Country ìµœë¹ˆê°’: usa\n",
      "\n",
      "[Books ì •ì œ]\n",
      "- ì¶œíŒì—°ë„ ì´ìƒì¹˜: 3ê±´\n",
      "- ì‚¬ìš©ëœ ì¤‘ì•™ ì—°ë„: 1996\n",
      "- Language ê²°ì¸¡ì¹˜: 67,227ê±´\n",
      "- Category ê²°ì¸¡ì¹˜: 68,851ê±´\n",
      "- Author ê²°ì¸¡ì¹˜: 1ê±´\n",
      "\n",
      "[Ratings ì •ì œ]\n",
      "- Train ì¤‘ë³µ ì œê±°: 0ê±´\n",
      "- Test ì¤‘ë³µ ì œê±°: 0ê±´\n",
      "\n",
      "==========\n",
      "\n",
      "ğŸ‰ ğŸ‰ ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class DataCleaningPipeline:\n",
    "\n",
    "    def __init__(self, data_path: str, output_path: str = None):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path or data_path\n",
    "        self.cleaning_report = {}\n",
    "        self.isbn_parser = LanguageParserByISBN()\n",
    "\n",
    "    def load_data(\n",
    "        self,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        print(\"> ì›ë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° >\")\n",
    "        users = pd.read_csv(self.data_path + \"users.csv\")\n",
    "        books = pd.read_csv(self.data_path + \"books.csv\")\n",
    "        train = pd.read_csv(self.data_path + \"train_ratings.csv\")\n",
    "        test = pd.read_csv(self.data_path + \"test_ratings.csv\")\n",
    "\n",
    "        self.cleaning_report[\"original_sizes\"] = {\n",
    "            \"users\": len(users),\n",
    "            \"books\": len(books),\n",
    "            \"train\": len(train),\n",
    "            \"test\": len(test),\n",
    "        }\n",
    "\n",
    "        print(f\"- Users: {len(users):,} rows\")\n",
    "        print(f\"- Books: {len(books):,} rows\")\n",
    "        print(f\"- Train: {len(train):,} rows\")\n",
    "        print(f\"- Test: {len(test):,} rows\\n\")\n",
    "\n",
    "        return users, books, train, test\n",
    "\n",
    "    # ë‚˜ì´ ë°ì´í„° ì •ì œ\n",
    "    def clean_age(self, users: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ê²°ì¸¡ì¹˜ -> ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´ (í‰ê· ì€ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ë‹ˆê¹Œ)\n",
    "        \"\"\"\n",
    "        print(\"> User Age ì •ì œ ì‹œì‘\\n\")\n",
    "\n",
    "        users = users.copy()\n",
    "        original_missing = users[\"age\"].isnull().sum()\n",
    "\n",
    "        # ì¤‘ê°„ê°’ ê³„ì‚°\n",
    "        reasonable_age = users[\"age\"][(users[\"age\"] >= 5) & (users[\"age\"] <= 100)]\n",
    "        median_age = reasonable_age.median()\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ â†’ ì¤‘ì•™ê°’\n",
    "        users[\"age\"].fillna(median_age, inplace=True)\n",
    "        print(f\"** ê²°ì¸¡ì¹˜ {original_missing:,}ê±´ â†’ ì¤‘ì•™ê°’({median_age:.0f}ì„¸)ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        self.cleaning_report[\"age\"] = {\n",
    "            \"original_missing\": original_missing,\n",
    "            \"median_used\": median_age,\n",
    "        }\n",
    "\n",
    "        print(f\"> Age ì •ì œ ì™„ë£Œ (ì¤‘ì•™ê°’: {median_age:.0f}ì„¸)\\n\")\n",
    "        return users\n",
    "\n",
    "    # ìœ„ì¹˜ ë°ì´í„° ì •ì œ\n",
    "    def clean_location(self, users: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1. 'n/a, n/a, n/a' â†’ NaNìœ¼ë¡œ ë³€í™˜\n",
    "        2. Country/State/City íŒŒì‹±\n",
    "        3. ê²°ì¸¡ì¹˜ ë³´ì™„ (State ìˆìœ¼ë©´ Country ì¶”ë¡ )\n",
    "        \"\"\"\n",
    "        print(\"> User Location ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        users = users.copy()\n",
    "\n",
    "        def split_location(x: str) -> list:\n",
    "            res = x.split(\",\")\n",
    "            res = [i.strip().lower() for i in res]\n",
    "            res = [regex.sub(r\"[^a-zA-Z/ ]\", \"\", i) for i in res]\n",
    "            res = [i if i not in [\"n/a\", \"\"] else np.nan for i in res]\n",
    "            res.reverse()\n",
    "\n",
    "            for i in range(len(res) - 1, 0, -1):\n",
    "                if (res[i] in res[:i]) and (not pd.isna(res[i])):\n",
    "                    res.pop(i)\n",
    "            return res\n",
    "\n",
    "        # Location íŒŒì‹±\n",
    "        users[\"location_list\"] = users[\"location\"].apply(split_location)\n",
    "        users[\"location_country\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[0] if len(x) > 0 else np.nan\n",
    "        )\n",
    "        users[\"location_state\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[1] if len(x) > 1 else np.nan\n",
    "        )\n",
    "        users[\"location_city\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[2] if len(x) > 2 else np.nan\n",
    "        )\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ ë³´ì™„\n",
    "        filled_countries = 0\n",
    "        filled_states = 0\n",
    "\n",
    "        for idx, row in users.iterrows():\n",
    "            # StateëŠ” ìˆëŠ”ë° Countryê°€ ì—†ëŠ” ê²½ìš°\n",
    "            if (not pd.isna(row[\"location_state\"])) and pd.isna(\n",
    "                row[\"location_country\"]\n",
    "            ):\n",
    "                fill_country = users[users[\"location_state\"] == row[\"location_state\"]][\n",
    "                    \"location_country\"\n",
    "                ].mode()\n",
    "                if len(fill_country) > 0:\n",
    "                    users.loc[idx, \"location_country\"] = fill_country[0]\n",
    "                    filled_countries += 1\n",
    "\n",
    "            # CityëŠ” ìˆëŠ”ë° Stateê°€ ì—†ëŠ” ê²½ìš°\n",
    "            elif (not pd.isna(row[\"location_city\"])) and pd.isna(row[\"location_state\"]):\n",
    "                if not pd.isna(row[\"location_country\"]):\n",
    "                    fill_state = users[\n",
    "                        (users[\"location_country\"] == row[\"location_country\"])\n",
    "                        & (users[\"location_city\"] == row[\"location_city\"])\n",
    "                    ][\"location_state\"].mode()\n",
    "                    if len(fill_state) > 0:\n",
    "                        users.loc[idx, \"location_state\"] = fill_state[0]\n",
    "                        filled_states += 1\n",
    "\n",
    "        # ë‚¨ì€ê±´ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        mode_country = (\n",
    "            users[\"location_country\"].mode()[0]\n",
    "            if not users[\"location_country\"].mode().empty\n",
    "            else \"unknown\"\n",
    "        )\n",
    "        original_country_missing = users[\"location_country\"].isnull().sum()\n",
    "        users[\"location_country\"].fillna(mode_country, inplace=True)\n",
    "\n",
    "        print(f\"** Country: {filled_countries}ê±´ ë³µì›\")\n",
    "        print(\n",
    "            f\"** Country: ìµœë¹ˆê°’({mode_country})ìœ¼ë¡œ {original_country_missing - filled_countries}ê±´ ëŒ€ì²´\"\n",
    "        )\n",
    "        print(f\"** State: {filled_states}ê±´ ë³µì›\")\n",
    "\n",
    "        users = users.drop([\"location\", \"location_list\"], axis=1)\n",
    "\n",
    "        self.cleaning_report[\"location\"] = {\n",
    "            \"filled_countries\": filled_countries,\n",
    "            \"filled_states\": filled_states,\n",
    "            \"mode_country\": mode_country,\n",
    "        }\n",
    "\n",
    "        print(f\"> Location ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return users\n",
    "\n",
    "    # ì±… ë°ì´í„° ì •ì œ\n",
    "    def clean_books(self, books: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1. year_of_publication: 1900 ë¯¸ë§Œ ë˜ëŠ” 2025 ì´ˆê³¼ â†’ ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        2. language: ê²°ì¸¡ì¹˜ â†’ ISBN ë²ˆí˜¸ ê¸°ë°˜ ì¶”ë¡  â†’ ì¶”ë¡  ì‹¤íŒ¨ ì‹œ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        3. category: ê²°ì¸¡ì¹˜ â†’ 'unknown'\n",
    "        4. book_author: ê²°ì¸¡ì¹˜ â†’ 'unknown'\n",
    "        \"\"\"\n",
    "        print(\"> Books ë°ì´í„° ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        books = books.copy()\n",
    "\n",
    "        # 1. Year of Publication ê²°ì¸¡ì¹˜ â†’ ì¤‘ê°„ê°’ìœ¼ë¡œ\n",
    "        reasonable_years = books[\"year_of_publication\"][\n",
    "            (books[\"year_of_publication\"] >= 1900)\n",
    "            & (books[\"year_of_publication\"] <= 2025)\n",
    "        ]\n",
    "        median_year = reasonable_years.median()\n",
    "\n",
    "        outlier_year_count = (\n",
    "            (books[\"year_of_publication\"] < 1900)\n",
    "            | (books[\"year_of_publication\"] > 2025)\n",
    "        ).sum()\n",
    "\n",
    "        books.loc[\n",
    "            (books[\"year_of_publication\"] < 1900)\n",
    "            | (books[\"year_of_publication\"] > 2025),\n",
    "            \"year_of_publication\",\n",
    "        ] = median_year\n",
    "\n",
    "        print(\n",
    "            f\"** ì¶œíŒì—°ë„: ë¹„ì •ìƒ ë²”ìœ„ {outlier_year_count}ê±´ â†’ ì¤‘ê°„ê°’({median_year:.0f})ìœ¼ë¡œ ëŒ€ì²´\"\n",
    "        )\n",
    "\n",
    "        # 2. Language ê²°ì¸¡ì¹˜ â†’ ISBN ë²ˆí˜¸ ê¸°ë°˜ ì¶”ë¡  â†’ ì¶”ë¡  ì‹¤íŒ¨ ì‹œ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        original_missing = books[\"language\"].isnull().sum()\n",
    "        print(f\"** ì›ë³¸ ê²°ì¸¡ì¹˜: {original_missing:,}ê±´\")\n",
    "\n",
    "        missing_mask = books[\"language\"].isnull()\n",
    "        books.loc[missing_mask, \"language\"] = books.loc[missing_mask, \"isbn\"].apply(\n",
    "            self.isbn_parser.infer_language_from_isbn\n",
    "        )\n",
    "\n",
    "        # ISBN ì¶”ë¡  í›„ ë‚¨ì€ ê²°ì¸¡ì¹˜\n",
    "        remaining_missing = books[\"language\"].isnull().sum()\n",
    "        isbn_filled = original_missing - remaining_missing\n",
    "\n",
    "        print(\n",
    "            f\"** ISBNìœ¼ë¡œ ìœ ì¶”í•œ ê°œìˆ˜: {isbn_filled:,}ê±´ ({isbn_filled/original_missing*100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # ë‚¨ì€ ê²°ì¸¡ì¹˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        if remaining_missing > 0:\n",
    "            mode_language = (\n",
    "                books[\"language\"].mode()[0]\n",
    "                if not books[\"language\"].mode().empty\n",
    "                else \"en\"\n",
    "            )\n",
    "            books[\"language\"].fillna(mode_language, inplace=True)\n",
    "            print(f\"** ìµœë¹ˆê°’({mode_language})ìœ¼ë¡œ ë‚˜ë¨¸ì§€ {remaining_missing:,}ê±´ ëŒ€ì²´\")\n",
    "\n",
    "        # ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "        if isbn_filled > 0:\n",
    "            filled_langs = books.loc[missing_mask, \"language\"].value_counts().head(5)\n",
    "            print(f\"\\n  [ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸ (Top 5)]\")\n",
    "            for lang, count in filled_langs.items():\n",
    "                print(f\"- {lang}: {count:,}ê±´\")\n",
    "\n",
    "        # 3. Category ê²°ì¸¡ì¹˜ â†’ unknown\n",
    "        def str2list(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            if x[0] != \"[\":\n",
    "                return x.split(\", \")\n",
    "            return x[1:-1].split(\", \")\n",
    "\n",
    "        books[\"category\"] = books[\"category\"].apply(\n",
    "            lambda x: str2list(x)[0] if not pd.isna(x) else \"unknown\"\n",
    "        )\n",
    "        category_missing = (books[\"category\"] == \"unknown\").sum()\n",
    "        print(f\"** Category: ê²°ì¸¡ì¹˜ {category_missing:,}ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        # 4. Book Author ê²°ì¸¡ì¹˜ â†’ unknown\n",
    "        author_missing = books[\"book_author\"].isnull().sum()\n",
    "        books[\"book_author\"].fillna(\"unknown\", inplace=True)\n",
    "        print(f\"** Book Author: ê²°ì¸¡ì¹˜ {author_missing}ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        self.cleaning_report[\"books\"] = {\n",
    "            \"outlier_year_count\": outlier_year_count,\n",
    "            \"median_year\": median_year,\n",
    "            \"language_missing\": original_missing,\n",
    "            \"category_missing\": category_missing,\n",
    "            \"author_missing\": author_missing,\n",
    "        }\n",
    "\n",
    "        print(f\"> Books ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return books\n",
    "\n",
    "    # í‰ì  ë°ì´í„° ì •ì œ\n",
    "    def clean_ratings(\n",
    "        self, train: pd.DataFrame, test: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "        \"\"\"\n",
    "        print(\"> Ratings ë°ì´í„° ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        train = train.copy()\n",
    "        test = test.copy()\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° (user_id + isbn ê¸°ì¤€)\n",
    "        train_duplicates = train.duplicated(subset=[\"user_id\", \"isbn\"]).sum()\n",
    "        if train_duplicates > 0:\n",
    "            train = train.drop_duplicates(subset=[\"user_id\", \"isbn\"], keep=\"last\")\n",
    "            print(f\"** Train: ì¤‘ë³µ ë°ì´í„° {train_duplicates}ê±´ ì œê±°\")\n",
    "\n",
    "        test_duplicates = test.duplicated(subset=[\"user_id\", \"isbn\"]).sum()\n",
    "        if test_duplicates > 0:\n",
    "            test = test.drop_duplicates(subset=[\"user_id\", \"isbn\"], keep=\"last\")\n",
    "            print(f\"** Test: ì¤‘ë³µ ë°ì´í„° {test_duplicates}ê±´ ì œê±°\")\n",
    "\n",
    "        self.cleaning_report[\"ratings\"] = {\n",
    "            \"train_duplicates\": train_duplicates,\n",
    "            \"test_duplicates\": test_duplicates,\n",
    "        }\n",
    "\n",
    "        print(f\"> Ratings ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return train, test\n",
    "\n",
    "    def save_cleaned_data(\n",
    "        self,\n",
    "        users: pd.DataFrame,\n",
    "        books: pd.DataFrame,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "    ):\n",
    "        print(\">>> ì •ì œëœ ë°ì´í„° ì €ì¥ ì‹œì‘\")\n",
    "\n",
    "        additional_path = (\n",
    "            self.output_path + \"v2/\"\n",
    "        )  # TODO: ë°ì´í„° ë²„ì „ì— ë”°ë¼ íŒŒì¼ëª… ìˆ˜ì •í•˜ê¸°\n",
    "        os.makedirs(additional_path, exist_ok=True)\n",
    "        print(f\"** ì €ì¥ ê²½ë¡œ: {additional_path}\")\n",
    "\n",
    "        # ì €ì¥ ì‹œ, Location ì»¬ëŸ¼ ì¬ê²°í•©\n",
    "        if all(\n",
    "            col in users.columns\n",
    "            for col in [\"location_country\", \"location_state\", \"location_city\"]\n",
    "        ):\n",
    "\n",
    "            def combine_location(row):\n",
    "                parts = []\n",
    "\n",
    "                # city, state, country\n",
    "                if pd.notna(row[\"location_city\"]) and row[\"location_city\"] != \"unknown\":\n",
    "                    parts.append(str(row[\"location_city\"]))\n",
    "\n",
    "                if (\n",
    "                    pd.notna(row[\"location_state\"])\n",
    "                    and row[\"location_state\"] != \"unknown\"\n",
    "                ):\n",
    "                    parts.append(str(row[\"location_state\"]))\n",
    "\n",
    "                if (\n",
    "                    pd.notna(row[\"location_country\"])\n",
    "                    and row[\"location_country\"] != \"unknown\"\n",
    "                ):\n",
    "                    parts.append(str(row[\"location_country\"]))\n",
    "\n",
    "                # ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "                if not parts:\n",
    "                    return \"unknown, unknown, unknown\"\n",
    "\n",
    "                return \", \".join(parts)\n",
    "\n",
    "            users[\"location\"] = users.apply(combine_location, axis=1)\n",
    "\n",
    "            # íŒŒì‹±ëœ ì»¬ëŸ¼ ì œê±°\n",
    "            users = users.drop(\n",
    "                [\"location_country\", \"location_state\", \"location_city\"], axis=1\n",
    "            )\n",
    "\n",
    "        users.to_csv(additional_path + \"users.csv\", index=False)\n",
    "        books.to_csv(additional_path + \"books.csv\", index=False)\n",
    "        train.to_csv(additional_path + \"train_ratings.csv\", index=False)\n",
    "        test.to_csv(additional_path + \"test_ratings.csv\", index=False)\n",
    "\n",
    "        print(f\"  - {additional_path}users.csv\")\n",
    "        print(f\"  - {additional_path}books.csv\")\n",
    "        print(f\"  - {additional_path}train_ratings.csv\")\n",
    "        print(f\"  - {additional_path}test_ratings.csv\")\n",
    "\n",
    "        print(\"\\n<<< ì €ì¥ ì™„ë£Œ! >>>\\n\")\n",
    "\n",
    "    def generate_cleaning_report(self):\n",
    "        print(\"=\" * 10)\n",
    "        print(\" \" * 20 + \">>> ë°ì´í„° ì •ì œ ê²°ê³¼ >>> \")\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "        print(\"\\n[ì›ë³¸ ë°ì´í„° í¬ê¸°]\")\n",
    "        for key, value in self.cleaning_report[\"original_sizes\"].items():\n",
    "            print(f\"  - {key.capitalize()}: {value:,} rows\")\n",
    "\n",
    "        print(\"\\n[Age ì •ì œ]\")\n",
    "        age_report = self.cleaning_report.get(\"age\", {})\n",
    "        print(f\"- ê²°ì¸¡ì¹˜ ëŒ€ì²´: {age_report.get('original_missing', 0):,}ê±´\")\n",
    "\n",
    "        print(\"\\n[Location ì •ì œ]\")\n",
    "        loc_report = self.cleaning_report.get(\"location\", {})\n",
    "        print(f\"- Country ë³µì›: {loc_report.get('filled_countries', 0)}ê±´\")\n",
    "        print(f\"- State ë³µì›: {loc_report.get('filled_states', 0)}ê±´\")\n",
    "        print(f\"- Country ìµœë¹ˆê°’: {loc_report.get('mode_country', 'N/A')}\")\n",
    "\n",
    "        print(\"\\n[Books ì •ì œ]\")\n",
    "        books_report = self.cleaning_report.get(\"books\", {})\n",
    "        print(f\"- ì¶œíŒì—°ë„ ì´ìƒì¹˜: {books_report.get('outlier_year_count', 0)}ê±´\")\n",
    "        print(f\"- ì‚¬ìš©ëœ ì¤‘ì•™ ì—°ë„: {books_report.get('median_year', 0):.0f}\")\n",
    "        print(f\"- Language ê²°ì¸¡ì¹˜: {books_report.get('language_missing', 0):,}ê±´\")\n",
    "        print(f\"- Category ê²°ì¸¡ì¹˜: {books_report.get('category_missing', 0):,}ê±´\")\n",
    "        print(f\"- Author ê²°ì¸¡ì¹˜: {books_report.get('author_missing', 0)}ê±´\")\n",
    "\n",
    "        print(\"\\n[Ratings ì •ì œ]\")\n",
    "        ratings_report = self.cleaning_report.get(\"ratings\", {})\n",
    "        print(f\"- Train ì¤‘ë³µ ì œê±°: {ratings_report.get('train_duplicates', 0)}ê±´\")\n",
    "        print(f\"- Test ì¤‘ë³µ ì œê±°: {ratings_report.get('test_duplicates', 0)}ê±´\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 10)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"ì „ì²´ ì‹¤í–‰\"\"\"\n",
    "\n",
    "        # 1. ë°ì´í„° ë¡œë“œ\n",
    "        users, books, train, test = self.load_data()\n",
    "\n",
    "        # 2. Users ì •ì œ\n",
    "        users = self.clean_age(users)\n",
    "        users = self.clean_location(users)\n",
    "\n",
    "        # 3. Books ì •ì œ\n",
    "        books = self.clean_books(books)\n",
    "\n",
    "        # 4. Ratings ì •ì œ\n",
    "        train, test = self.clean_ratings(train, test)\n",
    "\n",
    "        # 5. ì €ì¥\n",
    "        self.save_cleaned_data(users, books, train, test)\n",
    "\n",
    "        # 6. ë³´ê³ ì„œ ì¶œë ¥\n",
    "        self.generate_cleaning_report()\n",
    "\n",
    "        return users, books, train, test\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    DATA_PATH = \"/data/ephemeral/home/sojin/data/\"\n",
    "\n",
    "    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    pipeline = DataCleaningPipeline(DATA_PATH)\n",
    "    users_cleaned, books_cleaned, train_cleaned, test_cleaned = pipeline.run()\n",
    "\n",
    "    print(\"\\nğŸ‰ ğŸ‰ ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

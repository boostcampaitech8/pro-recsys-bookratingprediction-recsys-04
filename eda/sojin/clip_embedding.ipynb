{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c55850a",
   "metadata": {},
   "source": [
    "# CLIP 을 활용해 책 표지 임베딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa3f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4e_kwljs\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4e_kwljs\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from clip==1.0) (2.9.1)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wcwidth in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torch->clip==1.0) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Requirement already satisfied: numpy in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /data/ephemeral/home/sojin/.venv/lib/python3.10/site-packages (from torchvision->clip==1.0) (12.0.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369549 sha256=83add7418adf64a9b2e198b0783b14824ce35d040df2126effa0e8951af0b676\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jo4wqkih/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, torchvision, clip\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [clip][32m1/3\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed clip-1.0 ftfy-6.3.1 torchvision-0.24.1\n"
     ]
    }
   ],
   "source": [
    "!uv pip install git+https://github.com/openai/CLIP.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6849bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722e9f8",
   "metadata": {},
   "source": [
    "## CLIP 임베딩\n",
    "책 표지 이미지를 CLIP으로 임베딩하여 평점 데이터에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83996f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** device: cuda\n",
      "> CLIP 모델 (ViT-B/32) 로딩 시작\n",
      "> CLIP 모델 로딩 완료\n",
      "\n",
      "==========\n",
      "CLIP 이미지 임베딩 파이프라인 시작\n",
      "==========\n",
      "\n",
      "> 기존 매핑 파일이 있음!!! (/data/ephemeral/home/sojin/data/v5/book_image_map.csv)\n",
      "\n",
      "> CLIP 임베딩 생성 시작\n",
      "\n",
      "** 처리할 이미지: 149,522개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  임베딩 생성: 100%|██████████| 4673/4673 [09:09<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 임베딩 생성 결과:\n",
      "- 성공: 149,522개\n",
      "- 실패: 0개\n",
      "- dimension: 512\n",
      "> 임베딩 저장 완료 (경로: /data/ephemeral/home/sojin/data/v5/clip_image_embeddings.csv)\n",
      "\n",
      "[Train 데이터 병합]\n",
      "> 평점 데이터와 임베딩 병합 시작\n",
      "\n",
      ">> 병합 결과\n",
      "* 전체 평점 데이터: 306,795건\n",
      "* 이미지 매칭 성공: 281,590건 (91.78%)\n",
      "* 이미지 매칭 실패: 25,205건 (8.22%)\n",
      "\n",
      "<결측치 처리> 매칭되지 않은 25,205건은 임베딩을 0으로 채움......\n",
      "> 병합 결과 저장 (경로: /data/ephemeral/home/sojin/data/v5/train_ratings_with_clip.csv)\n",
      "\n",
      "\n",
      "[Test 데이터 병합]\n",
      "> 평점 데이터와 임베딩 병합 시작\n",
      "\n",
      ">> 병합 결과\n",
      "* 전체 평점 데이터: 76,699건\n",
      "* 이미지 매칭 성공: 70,467건 (91.87%)\n",
      "* 이미지 매칭 실패: 6,232건 (8.13%)\n",
      "\n",
      "<결측치 처리> 매칭되지 않은 6,232건은 임베딩을 0으로 채움......\n",
      "> 병합 결과 저장 (경로: /data/ephemeral/home/sojin/data/v5/test_ratings_with_clip.csv)\n",
      "\n",
      "\n",
      "==========\n",
      ">> 파이프라인 완료!\n",
      "==========\n",
      "\n",
      "\n",
      ">> 모든 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "class CLIPImageEmbedder:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        data_path: str,\n",
    "        model_name: str = \"ViT-B/32\",\n",
    "        device: Optional[str] = None,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        print(f\"** device: {self.device}\")\n",
    "\n",
    "        # CLIP 모델 로드\n",
    "        print(f\"> CLIP 모델 ({model_name}) 로딩 시작\")\n",
    "        self.model, self.preprocess = clip.load(model_name, device=self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"> CLIP 모델 로딩 완료\\n\")\n",
    "\n",
    "    # ISBN-이미지 매핑\n",
    "    def create_image_mapping(self) -> pd.DataFrame:\n",
    "        output_path = self.data_path + \"book_image_map.csv\"\n",
    "\n",
    "        print(\"> ISBN-이미지 매핑 시작\")\n",
    "\n",
    "        if not os.path.exists(self.image_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"이미지 디렉토리를 찾을 수 없습니다: {self.image_dir}\"\n",
    "            )\n",
    "\n",
    "        image_files = os.listdir(self.image_dir)\n",
    "        rows = []\n",
    "        failed_files = []\n",
    "\n",
    "        for fname in tqdm(image_files, desc=\"  이미지 파일 스캔\"):\n",
    "            # mac 숨김파일 건너뛰기\n",
    "            if fname.startswith(\"._\") or fname.startswith(\".\"):\n",
    "                continue\n",
    "\n",
    "            if fname.lower().endswith((\".jpg\")):\n",
    "                match = re.match(r\"(\\d+)\", fname)\n",
    "                if match:\n",
    "                    isbn = match.group(1)\n",
    "                    isbn = isbn.zfill(10)\n",
    "                    path = os.path.join(self.image_dir, fname)\n",
    "\n",
    "                    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "                        rows.append({\"isbn\": isbn, \"image_path\": path})\n",
    "                    else:\n",
    "                        failed_files.append(fname)\n",
    "                else:\n",
    "                    failed_files.append(fname)\n",
    "\n",
    "        df_img = pd.DataFrame(rows)\n",
    "        df_img[\"isbn\"] = df_img[\"isbn\"].astype(str)\n",
    "\n",
    "        original_len = len(df_img)\n",
    "        df_img = df_img.drop_duplicates(subset=[\"isbn\"], keep=\"first\")\n",
    "\n",
    "        print(f\"\\n> ISBN-이미지 매핑 결과:\")\n",
    "        print(f\"- 총 이미지 개수: {len(image_files):,}개\")\n",
    "        print(f\"- 매핑 성공: {len(df_img):,}개\")\n",
    "        print(f\"- 중복 제거: {original_len - len(df_img):,}개\")\n",
    "        print(f\"- 매핑 실패: {len(failed_files):,}개\")\n",
    "\n",
    "        if failed_files and len(failed_files) <= 10:\n",
    "            print(f\"\\n...... 매핑 실패 파일 예시: {failed_files[:5]}\")\n",
    "\n",
    "        df_img.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"> 매핑 파일 저장 완료 (경로: {output_path})\\n\")\n",
    "\n",
    "        return df_img\n",
    "\n",
    "    def _load_and_preprocess_batch(\n",
    "        self, image_paths: List[str]\n",
    "    ) -> Tuple[torch.Tensor, List[str]]:\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "\n",
    "        for path in image_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                img_tensor = self.preprocess(img)\n",
    "                images.append(img_tensor)\n",
    "                valid_paths.append(path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"...... 이미지 로딩 실패: {path} - {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if len(images) == 0:\n",
    "            return None, []\n",
    "\n",
    "        batch = torch.stack(images).to(self.device)\n",
    "        return batch, valid_paths\n",
    "\n",
    "    def generate_embeddings(\n",
    "        self,\n",
    "        resume: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        print(\"> CLIP 임베딩 생성 시작\\n\")\n",
    "\n",
    "        mapping_csv = self.data_path + \"book_image_map.csv\"\n",
    "        output_csv = self.data_path + \"clip_image_embeddings.csv\"\n",
    "\n",
    "        if not os.path.exists(mapping_csv):\n",
    "            raise FileNotFoundError(f\"매핑 파일을 찾을 수 없습니다: {mapping_csv}\")\n",
    "\n",
    "        df = pd.read_csv(mapping_csv, dtype={\"isbn\": str})\n",
    "        print(f\"** 처리할 이미지: {len(df):,}개\")\n",
    "\n",
    "        # Resume 처리\n",
    "        processed_isbns = set()\n",
    "        if resume and os.path.exists(output_csv):\n",
    "            existing_df = pd.read_csv(output_csv)\n",
    "            processed_isbns = set(existing_df[\"isbn\"].astype(str))\n",
    "\n",
    "            df = df[~df[\"isbn\"].astype(str).isin(processed_isbns)].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"** 기존 임베딩 발견: {len(processed_isbns):,}개 .... 남은 작업 {len(df):,}개 (이어서 처리 시작)\"\n",
    "            )\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"> 모든 이미지 처리 완료\")\n",
    "            return pd.read_csv(output_csv)\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_isbns = []\n",
    "        failed_count = 0\n",
    "\n",
    "        num_batches = (len(df) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(\n",
    "                range(0, len(df), self.batch_size),\n",
    "                total=num_batches,\n",
    "                desc=\"  임베딩 생성\",\n",
    "            ):\n",
    "\n",
    "                batch_df = df.iloc[i : i + self.batch_size]\n",
    "                batch_paths = batch_df[\"image_path\"].tolist()\n",
    "\n",
    "                batch_images, valid_paths = self._load_and_preprocess_batch(batch_paths)\n",
    "\n",
    "                if batch_images is None:\n",
    "                    failed_count += len(batch_paths)\n",
    "                    continue\n",
    "\n",
    "                failed_in_batch = len(batch_paths) - len(valid_paths)\n",
    "                failed_count += failed_in_batch\n",
    "\n",
    "                embeddings = self.model.encode_image(batch_images)\n",
    "                embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "                embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "                for idx, path in enumerate(valid_paths):\n",
    "                    original_idx = batch_df[batch_df[\"image_path\"] == path].index[0]\n",
    "                    isbn = batch_df.loc[original_idx, \"isbn\"]\n",
    "\n",
    "                    all_embeddings.append(embeddings_np[idx])\n",
    "                    all_isbns.append(isbn)\n",
    "\n",
    "        emb_array = np.array(all_embeddings)\n",
    "        emb_dim = emb_array.shape[1]\n",
    "\n",
    "        # 결과 저장\n",
    "        emb_cols = {f\"clip_emb_{i}\": emb_array[:, i] for i in range(emb_dim)}\n",
    "        result_df = pd.DataFrame(emb_cols)\n",
    "        result_df.insert(0, \"isbn\", all_isbns)\n",
    "        result_df[\"isbn\"] = result_df[\"isbn\"].astype(str)\n",
    "\n",
    "        print(f\"\\n>>> 임베딩 생성 결과:\")\n",
    "        print(f\"- 성공: {len(result_df):,}개\")\n",
    "        print(f\"- 실패: {failed_count:,}개\")\n",
    "        print(f\"- dimension: {emb_dim}\")\n",
    "\n",
    "        if resume and os.path.exists(output_csv) and len(processed_isbns) > 0:\n",
    "            existing_df = pd.read_csv(output_csv)\n",
    "            result_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "            print(f\"\\n- 최종 총 개수: {len(result_df):,}개\")\n",
    "\n",
    "        result_df.to_csv(output_csv, index=False)\n",
    "        print(f\"> 임베딩 저장 완료 (경로: {output_csv})\\n\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def merge_with_ratings(\n",
    "        self,\n",
    "        ratings_csv: str,\n",
    "        output_csv: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        print(\"> 평점 데이터와 임베딩 병합 시작\\n\")\n",
    "\n",
    "        embeddings_csv = self.data_path + \"clip_image_embeddings.csv\"\n",
    "\n",
    "        ratings = pd.read_csv(ratings_csv)\n",
    "        embeddings = pd.read_csv(embeddings_csv, dtype={\"isbn\": str})\n",
    "\n",
    "        ratings[\"isbn\"] = ratings[\"isbn\"].astype(str).str.zfill(10)\n",
    "        embeddings[\"isbn\"] = embeddings[\"isbn\"].astype(str).str.zfill(10)\n",
    "\n",
    "        original_len = len(ratings)\n",
    "        merged = ratings.merge(embeddings, on=\"isbn\", how=\"left\")\n",
    "\n",
    "        emb_cols = [col for col in merged.columns if col.startswith(\"clip_emb_\")]\n",
    "        matched = merged[emb_cols[0]].notna().sum()\n",
    "        missing = merged[emb_cols[0]].isna().sum()\n",
    "\n",
    "        print(f\">> 병합 결과\")\n",
    "        print(f\"* 전체 평점 데이터: {original_len:,}건\")\n",
    "        print(f\"* 이미지 매칭 성공: {matched:,}건 ({matched/original_len*100:.2f}%)\")\n",
    "        print(f\"* 이미지 매칭 실패: {missing:,}건 ({missing/original_len*100:.2f}%)\")\n",
    "\n",
    "        if missing > 0:\n",
    "            print(\n",
    "                f\"\\n<결측치 처리> 매칭되지 않은 {missing:,}건은 임베딩을 0으로 채움......\"\n",
    "            )\n",
    "            merged[emb_cols] = merged[emb_cols].fillna(0)\n",
    "\n",
    "        merged.to_csv(output_csv, index=False)\n",
    "        print(f\"> 병합 결과 저장 (경로: {output_csv})\\n\")\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def run_full_pipeline(\n",
    "        self,\n",
    "        mapping_csv: str,\n",
    "        train_ratings_csv: str,\n",
    "        test_ratings_csv: str,\n",
    "        train_output_csv: str,\n",
    "        test_output_csv: str,\n",
    "        force_regenerate: bool = False,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        print(\"=\" * 10)\n",
    "        print(\"CLIP 이미지 임베딩 파이프라인 시작\")\n",
    "        print(\"=\" * 10 + \"\\n\")\n",
    "\n",
    "        # Step 1: 이미지 매핑\n",
    "        if force_regenerate or not os.path.exists(mapping_csv):\n",
    "            self.create_image_mapping()\n",
    "        else:\n",
    "            print(f\"> 기존 매핑 파일이 있음!!! ({mapping_csv})\\n\")\n",
    "\n",
    "        # Step 2: 임베딩 생성\n",
    "        embeddings_csv = self.data_path + \"clip_image_embeddings.csv\"\n",
    "        if force_regenerate or not os.path.exists(embeddings_csv):\n",
    "            self.generate_embeddings(resume=not force_regenerate)\n",
    "        else:\n",
    "            print(f\"> 기존 임베딩 파일이 있음!!! ({embeddings_csv})\\n\")\n",
    "\n",
    "        # Step 3: Train 병합\n",
    "        print(\"[Train 데이터 병합]\")\n",
    "        train_merged = self.merge_with_ratings(train_ratings_csv, train_output_csv)\n",
    "\n",
    "        # Step 4: Test 병합\n",
    "        print(\"\\n[Test 데이터 병합]\")\n",
    "        test_merged = self.merge_with_ratings(test_ratings_csv, test_output_csv)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 10)\n",
    "        print(\">> 파이프라인 완료!\")\n",
    "        print(\"=\" * 10 + \"\\n\")\n",
    "\n",
    "        return train_merged, test_merged\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    IMAGE_DIR = \"/data/ephemeral/home/sojin/data/images\"  # TODO: 경로 확인\n",
    "    DATA_PATH = \"/data/ephemeral/home/sojin/data/v5/\"  # TODO: 경로 확인\n",
    "\n",
    "    embedder = CLIPImageEmbedder(\n",
    "        image_dir=IMAGE_DIR,\n",
    "        data_path=DATA_PATH,\n",
    "        model_name=\"ViT-B/32\",\n",
    "        device=\"cuda\",\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    # 전체 파이프라인 한번에 실행 (train + test)\n",
    "    train_merged, test_merged = embedder.run_full_pipeline(\n",
    "        mapping_csv=DATA_PATH + \"book_image_map.csv\",\n",
    "        train_ratings_csv=DATA_PATH + \"train_ratings.csv\",\n",
    "        test_ratings_csv=DATA_PATH + \"test_ratings.csv\",\n",
    "        train_output_csv=DATA_PATH + \"train_ratings_with_clip.csv\",\n",
    "        test_output_csv=DATA_PATH + \"test_ratings_with_clip.csv\",\n",
    "        force_regenerate=False,\n",
    "    )\n",
    "\n",
    "    print(\"\\n>> 모든 작업 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

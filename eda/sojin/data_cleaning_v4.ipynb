{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6a8c64",
   "metadata": {},
   "source": [
    "# ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ \n",
    "## Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4ef667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.13 environment at: /data/ephemeral/home/sojin/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install sentence-transformers scikit-learn tqdm --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0b2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "from typing import Tuple, Dict\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bc5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ê²½ìš°ì—ë§Œ import (Sentence-BERTëŠ” ë¬´ê±°ìš°ë¯€ë¡œ)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    SBERT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SBERT_AVAILABLE = False\n",
    "    print(\"{WARNING!} sentence-transformers ë˜ëŠ” sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"Step 4(í´ëŸ¬ìŠ¤í„°ë§)ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "class AdvancedCategoryHandler:\n",
    "    \"\"\"ê³ ê¸‰ ì¹´í…Œê³ ë¦¬ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\"\"\"\n",
    "\n",
    "    def __init__(self, use_clustering: bool = True):\n",
    "        self.use_clustering = use_clustering and SBERT_AVAILABLE\n",
    "        self.category_keywords = self._build_category_keywords()\n",
    "        self.sbert_model = None\n",
    "\n",
    "    def _build_category_keywords(self):\n",
    "        \"\"\"ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì‚¬ì „\"\"\"\n",
    "        return {\n",
    "            \"Fiction\": [\n",
    "                \"novel\",\n",
    "                \"story\",\n",
    "                \"tales\",\n",
    "                \"fiction\",\n",
    "                \"romance\",\n",
    "                \"mystery\",\n",
    "                \"thriller\",\n",
    "                \"fantasy\",\n",
    "                \"adventure\",\n",
    "            ],\n",
    "            \"History\": [\n",
    "                \"history\",\n",
    "                \"historical\",\n",
    "                \"war\",\n",
    "                \"ancient\",\n",
    "                \"medieval\",\n",
    "                \"century\",\n",
    "                \"civilization\",\n",
    "            ],\n",
    "            \"Biography\": [\n",
    "                \"biography\",\n",
    "                \"autobiography\",\n",
    "                \"memoir\",\n",
    "                \"life of\",\n",
    "                \"story of\",\n",
    "            ],\n",
    "            \"Science\": [\n",
    "                \"science\",\n",
    "                \"physics\",\n",
    "                \"chemistry\",\n",
    "                \"biology\",\n",
    "                \"astronomy\",\n",
    "                \"scientific\",\n",
    "            ],\n",
    "            \"Technology\": [\n",
    "                \"computer\",\n",
    "                \"programming\",\n",
    "                \"software\",\n",
    "                \"technology\",\n",
    "                \"digital\",\n",
    "                \"internet\",\n",
    "                \"web\",\n",
    "                \"python\",\n",
    "                \"java\",\n",
    "                \"code\",\n",
    "            ],\n",
    "            \"Business\": [\n",
    "                \"business\",\n",
    "                \"management\",\n",
    "                \"marketing\",\n",
    "                \"finance\",\n",
    "                \"economics\",\n",
    "                \"entrepreneur\",\n",
    "            ],\n",
    "            \"Self-Help\": [\n",
    "                \"self-help\",\n",
    "                \"self help\",\n",
    "                \"motivation\",\n",
    "                \"success\",\n",
    "                \"improvement\",\n",
    "                \"guide to\",\n",
    "            ],\n",
    "            \"Children\": [\"children\", \"kids\", \"young\", \"picture book\", \"illustrated\"],\n",
    "            \"Cooking\": [\"cook\", \"recipe\", \"food\", \"kitchen\", \"culinary\", \"cuisine\"],\n",
    "            \"Art\": [\n",
    "                \"art\",\n",
    "                \"artist\",\n",
    "                \"painting\",\n",
    "                \"design\",\n",
    "                \"illustration\",\n",
    "                \"photography\",\n",
    "            ],\n",
    "            \"Religion\": [\n",
    "                \"bible\",\n",
    "                \"god\",\n",
    "                \"christian\",\n",
    "                \"islam\",\n",
    "                \"buddhist\",\n",
    "                \"prayer\",\n",
    "                \"spiritual\",\n",
    "            ],\n",
    "            \"Poetry\": [\"poem\", \"poetry\", \"verse\"],\n",
    "            \"Travel\": [\"travel\", \"guide\", \"journey\", \"destination\", \"tourist\"],\n",
    "            \"Health\": [\"health\", \"medical\", \"medicine\", \"fitness\", \"diet\", \"wellness\"],\n",
    "            \"Education\": [\"education\", \"teaching\", \"learning\", \"textbook\", \"study\"],\n",
    "        }\n",
    "\n",
    "    def fill_category_missing(self, books: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ë‹¤ë‹¨ê³„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\"\"\"\n",
    "        books = books.copy()\n",
    "        original_missing = books[\"category\"].isna().sum()\n",
    "\n",
    "        print(f\"\\n{'='*10}\")\n",
    "        print(f\"> Advanced Category ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘\")\n",
    "        print(f\"{'='*10}\")\n",
    "        print(\n",
    "            f\"ì›ë³¸ ê²°ì¸¡ì¹˜: {original_missing:,}ê°œ ({original_missing/len(books)*100:.2f}%)\\n\"\n",
    "        )\n",
    "\n",
    "        report = {\"original_missing\": original_missing}\n",
    "\n",
    "        # Step 1: ë™ì¼ ì €ì + ì¶œíŒì—°ë„ Â±2ë…„\n",
    "        books, step1_filled = self._fill_by_author_year(books)\n",
    "        report[\"step1_filled\"] = step1_filled\n",
    "\n",
    "        # Step 2: ë™ì¼ Publisher\n",
    "        books, step2_filled = self._fill_by_publisher(books)\n",
    "        report[\"step2_filled\"] = step2_filled\n",
    "\n",
    "        # Step 3: Book Title í‚¤ì›Œë“œ\n",
    "        books, step3_filled = self._fill_by_title_keywords(books)\n",
    "        report[\"step3_filled\"] = step3_filled\n",
    "\n",
    "        # Step 4: Sentence-BERT í´ëŸ¬ìŠ¤í„°ë§\n",
    "        if self.use_clustering:\n",
    "            books, step4_filled = self._fill_by_clustering(books)\n",
    "            report[\"step4_filled\"] = step4_filled\n",
    "        else:\n",
    "            step4_filled = 0\n",
    "            report[\"step4_filled\"] = 0\n",
    "\n",
    "        # Step 5: unknown\n",
    "        books[\"category\"].fillna(\"unknown\", inplace=True)\n",
    "        final_unknown = (books[\"category\"] == \"unknown\").sum()\n",
    "        report[\"final_unknown\"] = final_unknown\n",
    "\n",
    "        print(f\"{'='*10}\")\n",
    "        print(f\"> Category ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "        print(f\"{'='*10}\")\n",
    "\n",
    "        print(f\"Step 1 (ì €ì+ì—°ë„): {step1_filled:,}ê°œ\")\n",
    "        print(f\"Step 2 (Publisher): {step2_filled:,}ê°œ\")\n",
    "        print(f\"Step 3 (í‚¤ì›Œë“œ): {step3_filled:,}ê°œ\")\n",
    "        print(f\"Step 4 (í´ëŸ¬ìŠ¤í„°ë§): {step4_filled:,}ê°œ\")\n",
    "        print(f\"Step 5 (unknown): {final_unknown:,}ê°œ\")\n",
    "        print(\n",
    "            f\"ì´ í•´ê²°: {original_missing - final_unknown:,}ê°œ ({(original_missing - final_unknown)/original_missing*100:.2f}%)\\n\"\n",
    "        )\n",
    "\n",
    "        return books, report\n",
    "\n",
    "    def _fill_by_author_year(self, books: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        # Step 1: ë™ì¼ ì €ì + ì¶œíŒì—°ë„ Â±2ë…„\n",
    "        print(\"> Step 1: ë™ì¼ ì €ì + ì¶œíŒì—°ë„ Â±2ë…„ ì´ë‚´\")\n",
    "        books = books.copy()\n",
    "        missing_mask = books[\"category\"].isna()\n",
    "        filled_count = 0\n",
    "\n",
    "        for idx in tqdm(books[missing_mask].index, desc=\"  ì²˜ë¦¬ ì¤‘\"):\n",
    "            row = books.loc[idx]\n",
    "            if pd.isna(row[\"book_author\"]) or row[\"book_author\"] == \"unknown\":\n",
    "                continue\n",
    "\n",
    "            same_author = books[\n",
    "                (books[\"book_author\"] == row[\"book_author\"])\n",
    "                & (\n",
    "                    books[\"year_of_publication\"].between(\n",
    "                        row[\"year_of_publication\"] - 2, row[\"year_of_publication\"] + 2\n",
    "                    )\n",
    "                )\n",
    "                & (books[\"category\"].notna())\n",
    "            ]\n",
    "\n",
    "            if len(same_author) > 0:\n",
    "                mode_category = same_author[\"category\"].mode()\n",
    "                if len(mode_category) > 0:\n",
    "                    books.loc[idx, \"category\"] = mode_category[0]\n",
    "                    filled_count += 1\n",
    "\n",
    "        remaining = books[\"category\"].isna().sum()\n",
    "        print(f\"  âœ“ {filled_count:,}ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: {remaining:,}ê°œ\\n\")\n",
    "        return books, filled_count\n",
    "\n",
    "    def _fill_by_publisher(self, books: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        # Step 2: ë™ì¼ Publisher\n",
    "        print(\"> Step 2: ë™ì¼ Publisher\")\n",
    "        books = books.copy()\n",
    "        missing_mask = books[\"category\"].isna()\n",
    "        filled_count = 0\n",
    "\n",
    "        # Publisherë³„ ìµœë¹ˆ ì¹´í…Œê³ ë¦¬ ê³„ì‚°\n",
    "        publisher_category_map = {}\n",
    "        for publisher in books[missing_mask][\"publisher\"].unique():\n",
    "            if pd.isna(publisher):\n",
    "                continue\n",
    "            same_publisher = books[\n",
    "                (books[\"publisher\"] == publisher) & (books[\"category\"].notna())\n",
    "            ]\n",
    "            if len(same_publisher) > 0:\n",
    "                mode_category = same_publisher[\"category\"].mode()\n",
    "                if len(mode_category) > 0:\n",
    "                    publisher_category_map[publisher] = mode_category[0]\n",
    "\n",
    "        # ì ìš©\n",
    "        for idx in books[missing_mask].index:\n",
    "            publisher = books.loc[idx, \"publisher\"]\n",
    "            if not pd.isna(publisher) and publisher in publisher_category_map:\n",
    "                books.loc[idx, \"category\"] = publisher_category_map[publisher]\n",
    "                filled_count += 1\n",
    "\n",
    "        remaining = books[\"category\"].isna().sum()\n",
    "        print(f\"  âœ“ {filled_count:,}ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: {remaining:,}ê°œ\\n\")\n",
    "        return books, filled_count\n",
    "\n",
    "    def _fill_by_title_keywords(self, books: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        # Step 3: Title í‚¤ì›Œë“œ ë§¤ì¹­\n",
    "        print(\"> Step 3: Book Title í‚¤ì›Œë“œ ë§¤ì¹­\")\n",
    "        books = books.copy()\n",
    "        missing_mask = books[\"category\"].isna()\n",
    "        filled_count = 0\n",
    "\n",
    "        for idx in tqdm(books[missing_mask].index, desc=\"  ì²˜ë¦¬ ì¤‘\"):\n",
    "            title = books.loc[idx, \"book_title\"]\n",
    "            if pd.isna(title):\n",
    "                continue\n",
    "\n",
    "            title_lower = str(title).lower()\n",
    "            for category, keywords in self.category_keywords.items():\n",
    "                if any(keyword in title_lower for keyword in keywords):\n",
    "                    books.loc[idx, \"category\"] = category\n",
    "                    filled_count += 1\n",
    "                    break\n",
    "\n",
    "        remaining = books[\"category\"].isna().sum()\n",
    "        print(f\"  âœ“ {filled_count:,}ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: {remaining:,}ê°œ\\n\")\n",
    "        return books, filled_count\n",
    "\n",
    "    def _fill_by_clustering(\n",
    "        self, books: pd.DataFrame, n_clusters: int = 650\n",
    "    ) -> Tuple[pd.DataFrame, int]:\n",
    "\n",
    "        # Step 4: Sentence-BERT í´ëŸ¬ìŠ¤í„°ë§\n",
    "        print(f\"> Step 4: Sentence-BERT í´ëŸ¬ìŠ¤í„°ë§ (ëª©í‘œ: {n_clusters}ê°œ)\")\n",
    "        books = books.copy()\n",
    "        filled_count = 0\n",
    "\n",
    "        if self.sbert_model is None:\n",
    "            print(\"** ëª¨ë¸ ë¡œë”© ì‹œì‘\")\n",
    "            self.sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        books_with_category = books[books[\"category\"].notna()].copy()\n",
    "        unique_categories = books_with_category[\"category\"].unique()\n",
    "\n",
    "        if len(unique_categories) < n_clusters:\n",
    "            print(f\"**** ì¹´í…Œê³ ë¦¬ ìˆ˜({len(unique_categories)})ê°€ ëª©í‘œë³´ë‹¤ ì ì–´ ìŠ¤í‚µ\\n\")\n",
    "            return books, 0\n",
    "\n",
    "        print(f\"** ì„ë² ë”© ìƒì„± ì‹œì‘\")\n",
    "        category_embeddings = self.sbert_model.encode(\n",
    "            unique_categories.tolist(), show_progress_bar=False, batch_size=32\n",
    "        )\n",
    "\n",
    "        print(f\"** í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘\")\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, metric=\"cosine\", linkage=\"average\"\n",
    "        )\n",
    "        cluster_labels = clustering.fit_predict(category_embeddings)\n",
    "\n",
    "        category_to_cluster = dict(zip(unique_categories, cluster_labels))\n",
    "\n",
    "        # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì¹´í…Œê³ ë¦¬\n",
    "        cluster_representatives = {}\n",
    "        for cluster_id in range(n_clusters):\n",
    "            categories_in_cluster = [\n",
    "                cat for cat, cid in category_to_cluster.items() if cid == cluster_id\n",
    "            ]\n",
    "            if categories_in_cluster:\n",
    "                category_counts = books_with_category[\n",
    "                    books_with_category[\"category\"].isin(categories_in_cluster)\n",
    "                ][\"category\"].value_counts()\n",
    "                cluster_representatives[cluster_id] = category_counts.index[0]\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        missing_mask = books[\"category\"].isna()\n",
    "        missing_books = books[missing_mask].copy()\n",
    "\n",
    "        if len(missing_books) > 0:\n",
    "            print(f\"** ê²°ì¸¡ì¹˜ {len(missing_books):,}ê°œ ì²˜ë¦¬ ì‹œì‘\")\n",
    "            missing_titles = missing_books[\"book_title\"].fillna(\"\").tolist()\n",
    "            missing_embeddings = self.sbert_model.encode(\n",
    "                missing_titles, show_progress_bar=False, batch_size=32\n",
    "            )\n",
    "\n",
    "            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬\n",
    "            cluster_centers = {}\n",
    "            for cluster_id in range(n_clusters):\n",
    "                categories_in_cluster = [\n",
    "                    cat for cat, cid in category_to_cluster.items() if cid == cluster_id\n",
    "                ]\n",
    "                if categories_in_cluster:\n",
    "                    cluster_indices = [\n",
    "                        i\n",
    "                        for i, cat in enumerate(unique_categories)\n",
    "                        if cat in categories_in_cluster\n",
    "                    ]\n",
    "                    cluster_centers[cluster_id] = category_embeddings[\n",
    "                        cluster_indices\n",
    "                    ].mean(axis=0)\n",
    "\n",
    "            # ë§¤ì¹­\n",
    "            for i, (idx, row) in enumerate(missing_books.iterrows()):\n",
    "                if pd.isna(row[\"book_title\"]) or str(row[\"book_title\"]).strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                similarities = {\n",
    "                    cluster_id: cosine_similarity([missing_embeddings[i]], [center])[0][\n",
    "                        0\n",
    "                    ]\n",
    "                    for cluster_id, center in cluster_centers.items()\n",
    "                }\n",
    "\n",
    "                best_cluster = max(similarities, key=similarities.get)\n",
    "                if best_cluster in cluster_representatives:\n",
    "                    books.loc[idx, \"category\"] = cluster_representatives[best_cluster]\n",
    "                    filled_count += 1\n",
    "\n",
    "        remaining = books[\"category\"].isna().sum()\n",
    "        print(f\"  âœ“ {filled_count:,}ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: {remaining:,}ê°œ\\n\")\n",
    "        return books, filled_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaddc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Languageë¥¼ ISBN ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class LanguageParserByISBN:\n",
    "    def __init__(self):\n",
    "        \"\"\"ISBN ì ‘ë‘ì‚¬ â†’ Language ë§¤í•‘\"\"\"\n",
    "        self.isbn_language_map = {\n",
    "            # ì˜ì–´ê¶Œ (0, 1)\n",
    "            \"0\": \"en\",\n",
    "            \"1\": \"en\",\n",
    "            # í”„ë‘ìŠ¤ì–´ê¶Œ (2)\n",
    "            \"2\": \"fr\",\n",
    "            # ë…ì¼ì–´ê¶Œ (3)\n",
    "            \"3\": \"de\",\n",
    "            # ì¼ë³¸ (4)\n",
    "            \"4\": \"ja\",\n",
    "            # ëŸ¬ì‹œì•„ (5)\n",
    "            \"5\": \"ru\",\n",
    "            # ì¤‘êµ­ (7)\n",
    "            \"7\": \"zh\",\n",
    "            # ì²´ì½”/ìŠ¬ë¡œë°”í‚¤ì•„ (80)\n",
    "            \"80\": \"cs\",\n",
    "            # ì¸ë„ (81)\n",
    "            \"81\": \"hi\",\n",
    "            # ë…¸ë¥´ì›¨ì´ (82)\n",
    "            \"82\": \"no\",\n",
    "            # í´ë€ë“œ (83)\n",
    "            \"83\": \"pl\",\n",
    "            # ìŠ¤í˜ì¸ (84)\n",
    "            \"84\": \"es\",\n",
    "            # ë¸Œë¼ì§ˆ (85)\n",
    "            \"85\": \"pt\",\n",
    "            # ìœ ê³ ìŠ¬ë¼ë¹„ì•„ (86)\n",
    "            \"86\": \"sr\",\n",
    "            # ë´ë§ˆí¬ (87)\n",
    "            \"87\": \"da\",\n",
    "            # ì´íƒˆë¦¬ì•„ (88)\n",
    "            \"88\": \"it\",\n",
    "            # í•œêµ­ (89)\n",
    "            \"89\": \"ko\",\n",
    "            # ë„¤ëœë€ë“œ (90)\n",
    "            \"90\": \"nl\",\n",
    "            # ìŠ¤ì›¨ë´ (91)\n",
    "            \"91\": \"sv\",\n",
    "            # í¬ë¥´íˆ¬ê°ˆ (972)\n",
    "            \"972\": \"pt\",\n",
    "            # í„°í‚¤ (975)\n",
    "            \"975\": \"tr\",\n",
    "            # ISBN-13 ë³€í™˜ ì ‘ë‘ì‚¬\n",
    "            \"978\": \"en\",\n",
    "            \"979\": \"en\",\n",
    "        }\n",
    "\n",
    "    def extract_isbn_prefix(self, isbn: str) -> str:\n",
    "        \"\"\"ISBNì—ì„œ Group Identifier ì¶”ì¶œ\"\"\"\n",
    "        if pd.isna(isbn):\n",
    "            return None\n",
    "\n",
    "        isbn_str = str(isbn).strip()\n",
    "\n",
    "        # ISBN-13ì¸ ê²½ìš° (978 ë˜ëŠ” 979ë¡œ ì‹œì‘)\n",
    "        if isbn_str.startswith((\"978\", \"979\")):\n",
    "            if len(isbn_str) >= 4:\n",
    "                return isbn_str[3]\n",
    "\n",
    "        # 3ìë¦¬ ì ‘ë‘ì‚¬ í™•ì¸ (ì˜ˆ: 972)\n",
    "        if len(isbn_str) >= 3 and isbn_str[:3] in self.isbn_language_map:\n",
    "            return isbn_str[:3]\n",
    "\n",
    "        # 2ìë¦¬ ì ‘ë‘ì‚¬ í™•ì¸ (ì˜ˆ: 80, 84)\n",
    "        if len(isbn_str) >= 2 and isbn_str[:2] in self.isbn_language_map:\n",
    "            return isbn_str[:2]\n",
    "\n",
    "        # 1ìë¦¬ ì ‘ë‘ì‚¬ (ì˜ˆ: 0, 1, 2, 3)\n",
    "        if len(isbn_str) >= 1 and isbn_str[0] in self.isbn_language_map:\n",
    "            return isbn_str[0]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def infer_language_from_isbn(self, isbn: str, default: str = \"en\") -> str:\n",
    "        \"\"\"ISBNìœ¼ë¡œë¶€í„° Language ì¶”ë¡ \"\"\"\n",
    "        prefix = self.extract_isbn_prefix(isbn)\n",
    "\n",
    "        if prefix is None:\n",
    "            return default\n",
    "\n",
    "        return self.isbn_language_map.get(prefix, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ì›ë³¸ ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "  - Users: 68,092 rows\n",
      "  - Books: 149,570 rows\n",
      "  - Train: 306,795 rows\n",
      "  - Test: 76,699 rows\n",
      "\n",
      "> User Age ì •ì œ ì‹œì‘\n",
      "\n",
      "  - ê²°ì¸¡ì¹˜ 27,833ê±´ â†’ ì¤‘ì•™ê°’(34ì„¸)ë¡œ ëŒ€ì²´\n",
      "> Age ì •ì œ ì™„ë£Œ (ì¤‘ì•™ê°’: 34ì„¸)\n",
      "\n",
      "> User Location ì •ì œ ì‹œì‘\n",
      "** Country: 586ê±´ ë³µì›\n",
      "** Country: ìµœë¹ˆê°’(usa)ìœ¼ë¡œ 932ê±´ ëŒ€ì²´\n",
      "** State: 1140ê±´ ë³µì›\n",
      "> Location ì •ì œ ì™„ë£Œ\n",
      "\n",
      "> Books ë°ì´í„° ì •ì œ ì‹œì‘\n",
      "** ì¶œíŒì—°ë„: ë¹„ì •ìƒ ë²”ìœ„ 3ê±´ â†’ ì¤‘ê°„ê°’(1996)ìœ¼ë¡œ ëŒ€ì²´\n",
      "** ì›ë³¸ ê²°ì¸¡ì¹˜: 67,227ê±´\n",
      "** ISBNìœ¼ë¡œ ìœ ì¶”í•œ ê°œìˆ˜: 67,227ê±´ (100.0%)\n",
      "\n",
      "  [ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸ (Top 5)]\n",
      "- en: 56,369ê±´\n",
      "- de: 5,284ê±´\n",
      "- fr: 2,420ê±´\n",
      "- es: 2,415ê±´\n",
      "- it: 355ê±´\n",
      "\n",
      "==========\n",
      "> Advanced Category ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘\n",
      "==========\n",
      "ì›ë³¸ ê²°ì¸¡ì¹˜: 0ê°œ (0.00%)\n",
      "\n",
      "> Step 1: ë™ì¼ ì €ì + ì¶œíŒì—°ë„ Â±2ë…„ ì´ë‚´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ì²˜ë¦¬ ì¤‘: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ 0ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: 0ê°œ\n",
      "\n",
      "> Step 2: ë™ì¼ Publisher\n",
      "  âœ“ 0ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: 0ê°œ\n",
      "\n",
      "> Step 3: Book Title í‚¤ì›Œë“œ ë§¤ì¹­\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ì²˜ë¦¬ ì¤‘: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ 0ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: 0ê°œ\n",
      "\n",
      "> Step 4: Sentence-BERT í´ëŸ¬ìŠ¤í„°ë§ (ëª©í‘œ: 650ê°œ)\n",
      "** ëª¨ë¸ ë¡œë”© ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** ì„ë² ë”© ìƒì„± ì‹œì‘\n",
      "** í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘\n",
      "  âœ“ 0ê°œ í•´ê²° | ë‚¨ì€ ê²°ì¸¡ì¹˜: 0ê°œ\n",
      "\n",
      "==========\n",
      "> Category ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\n",
      "==========\n",
      "Step 1 (ì €ì+ì—°ë„): 0ê°œ\n",
      "Step 2 (Publisher): 0ê°œ\n",
      "Step 3 (í‚¤ì›Œë“œ): 0ê°œ\n",
      "Step 4 (í´ëŸ¬ìŠ¤í„°ë§): 0ê°œ\n",
      "Step 5 (unknown): 0ê°œ\n",
      "ì´ í•´ê²°: 0ê°œ (nan%)\n",
      "\n",
      "** Book Author: ê²°ì¸¡ì¹˜ 1ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\n",
      "> Books ì •ì œ ì™„ë£Œ\n",
      "\n",
      "> Ratings ë°ì´í„° ì •ì œ ì‹œì‘\n",
      "> Ratings ì •ì œ ì™„ë£Œ\n",
      "\n",
      ">>> ì •ì œëœ ë°ì´í„° ì €ì¥ ì‹œì‘\n",
      "** ì €ì¥ ê²½ë¡œ: /data/ephemeral/home/sojin/data/v4/\n",
      "  - /data/ephemeral/home/sojin/data/v4/users.csv\n",
      "  - /data/ephemeral/home/sojin/data/v4/books.csv\n",
      "  - /data/ephemeral/home/sojin/data/v4/train_ratings.csv\n",
      "  - /data/ephemeral/home/sojin/data/v4/test_ratings.csv\n",
      "\n",
      "<<< ì €ì¥ ì™„ë£Œ! >>>\n",
      "\n",
      "==========\n",
      "                    >>> ë°ì´í„° ì •ì œ ê²°ê³¼ >>> \n",
      "==========\n",
      "\n",
      "[ì›ë³¸ ë°ì´í„° í¬ê¸°]\n",
      "  - Users: 68,092 rows\n",
      "  - Books: 149,570 rows\n",
      "  - Train: 306,795 rows\n",
      "  - Test: 76,699 rows\n",
      "\n",
      "[Age ì •ì œ]\n",
      "- ê²°ì¸¡ì¹˜ ëŒ€ì²´: 27,833ê±´\n",
      "\n",
      "[Location ì •ì œ]\n",
      "- Country ë³µì›: 586ê±´\n",
      "- State ë³µì›: 1140ê±´\n",
      "- Country ìµœë¹ˆê°’: usa\n",
      "\n",
      "[Books ì •ì œ]\n",
      "- ì¶œíŒì—°ë„ ì´ìƒì¹˜: 3ê±´\n",
      "- ì‚¬ìš©ëœ ì¤‘ì•™ ì—°ë„: 1996\n",
      "- Language ê²°ì¸¡ì¹˜: 67,227ê±´\n",
      "- Category ê²°ì¸¡ì¹˜: 0ê±´\n",
      "- Author ê²°ì¸¡ì¹˜: 1ê±´\n",
      "\n",
      "[Advanced Category ì²˜ë¦¬ ìƒì„¸]\n",
      "- ì›ë³¸ ê²°ì¸¡ì¹˜: 0ê±´\n",
      "- Step 1 (ì €ì+ì—°ë„): 0ê±´\n",
      "- Step 2 (Publisher): 0ê±´\n",
      "- Step 3 (í‚¤ì›Œë“œ): 0ê±´\n",
      "- Step 4 (í´ëŸ¬ìŠ¤í„°ë§): 0ê±´\n",
      "- ìµœì¢… unknown: 0ê±´\n",
      "\n",
      "[Ratings ì •ì œ]\n",
      "- Train ì¤‘ë³µ ì œê±°: 0ê±´\n",
      "- Test ì¤‘ë³µ ì œê±°: 0ê±´\n",
      "\n",
      "==========\n",
      "\n",
      "ğŸ‰ ğŸ‰ ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "class DataCleaningPipeline:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        output_path: str = None,\n",
    "        use_advanced_category: bool = True,\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path or data_path\n",
    "        self.cleaning_report = {}\n",
    "        self.use_advanced_category = use_advanced_category\n",
    "        self.isbn_parser = LanguageParserByISBN()\n",
    "        self.category_handler = (\n",
    "            AdvancedCategoryHandler() if use_advanced_category else None\n",
    "        )\n",
    "\n",
    "    def load_data(\n",
    "        self,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        print(\"> ì›ë³¸ ë°ì´í„° ë¡œë”© ì‹œì‘\")\n",
    "        users = pd.read_csv(self.data_path + \"users.csv\")\n",
    "        books = pd.read_csv(\n",
    "            \"/data/ephemeral/home/sojin/data/books_with_categories.csv\"\n",
    "        )  # TODO: ê²½ë¡œìˆ˜ì •\n",
    "        train = pd.read_csv(self.data_path + \"train_ratings.csv\")\n",
    "        test = pd.read_csv(self.data_path + \"test_ratings.csv\")\n",
    "\n",
    "        self.cleaning_report[\"original_sizes\"] = {\n",
    "            \"users\": len(users),\n",
    "            \"books\": len(books),\n",
    "            \"train\": len(train),\n",
    "            \"test\": len(test),\n",
    "        }\n",
    "\n",
    "        print(f\"- Users: {len(users):,} rows\")\n",
    "        print(f\"- Books: {len(books):,} rows\")\n",
    "        print(f\"- Train: {len(train):,} rows\")\n",
    "        print(f\"- Test: {len(test):,} rows\\n\")\n",
    "\n",
    "        return users, books, train, test\n",
    "\n",
    "    # ë‚˜ì´ ë°ì´í„° ì •ì œ\n",
    "    def clean_age(self, users: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ê²°ì¸¡ì¹˜ -> ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´ (í‰ê· ì€ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ë‹ˆê¹Œ)\n",
    "        \"\"\"\n",
    "        print(\"> User Age ì •ì œ ì‹œì‘\\n\")\n",
    "\n",
    "        users = users.copy()\n",
    "        original_missing = users[\"age\"].isnull().sum()\n",
    "\n",
    "        # ì¤‘ê°„ê°’ ê³„ì‚°\n",
    "        reasonable_age = users[\"age\"][(users[\"age\"] >= 5) & (users[\"age\"] <= 100)]\n",
    "        median_age = reasonable_age.median()\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ â†’ ì¤‘ì•™ê°’\n",
    "        users[\"age\"].fillna(median_age, inplace=True)\n",
    "        print(f\"  - ê²°ì¸¡ì¹˜ {original_missing:,}ê±´ â†’ ì¤‘ì•™ê°’({median_age:.0f}ì„¸)ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        self.cleaning_report[\"age\"] = {\n",
    "            \"original_missing\": original_missing,\n",
    "            \"median_used\": median_age,\n",
    "        }\n",
    "\n",
    "        print(f\"> Age ì •ì œ ì™„ë£Œ (ì¤‘ì•™ê°’: {median_age:.0f}ì„¸)\\n\")\n",
    "        return users\n",
    "\n",
    "    # ìœ„ì¹˜ ë°ì´í„° ì •ì œ\n",
    "    def clean_location(self, users: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1. 'n/a, n/a, n/a' â†’ NaNìœ¼ë¡œ ë³€í™˜\n",
    "        2. Country/State/City íŒŒì‹±\n",
    "        3. ê²°ì¸¡ì¹˜ ë³´ì™„ (State ìˆìœ¼ë©´ Country ì¶”ë¡ )\n",
    "        \"\"\"\n",
    "        print(\"> User Location ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        users = users.copy()\n",
    "\n",
    "        def split_location(x: str) -> list:\n",
    "            res = x.split(\",\")\n",
    "            res = [i.strip().lower() for i in res]\n",
    "            res = [regex.sub(r\"[^a-zA-Z/ ]\", \"\", i) for i in res]\n",
    "            res = [i if i not in [\"n/a\", \"\"] else np.nan for i in res]\n",
    "            res.reverse()\n",
    "\n",
    "            for i in range(len(res) - 1, 0, -1):\n",
    "                if (res[i] in res[:i]) and (not pd.isna(res[i])):\n",
    "                    res.pop(i)\n",
    "            return res\n",
    "\n",
    "        # Location íŒŒì‹±\n",
    "        users[\"location_list\"] = users[\"location\"].apply(split_location)\n",
    "        users[\"location_country\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[0] if len(x) > 0 else np.nan\n",
    "        )\n",
    "        users[\"location_state\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[1] if len(x) > 1 else np.nan\n",
    "        )\n",
    "        users[\"location_city\"] = users[\"location_list\"].apply(\n",
    "            lambda x: x[2] if len(x) > 2 else np.nan\n",
    "        )\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ ë³´ì™„\n",
    "        filled_countries = 0\n",
    "        filled_states = 0\n",
    "\n",
    "        for idx, row in users.iterrows():\n",
    "            # StateëŠ” ìˆëŠ”ë° Countryê°€ ì—†ëŠ” ê²½ìš°\n",
    "            if (not pd.isna(row[\"location_state\"])) and pd.isna(\n",
    "                row[\"location_country\"]\n",
    "            ):\n",
    "                fill_country = users[users[\"location_state\"] == row[\"location_state\"]][\n",
    "                    \"location_country\"\n",
    "                ].mode()\n",
    "                if len(fill_country) > 0:\n",
    "                    users.loc[idx, \"location_country\"] = fill_country[0]\n",
    "                    filled_countries += 1\n",
    "\n",
    "            # CityëŠ” ìˆëŠ”ë° Stateê°€ ì—†ëŠ” ê²½ìš°\n",
    "            elif (not pd.isna(row[\"location_city\"])) and pd.isna(row[\"location_state\"]):\n",
    "                if not pd.isna(row[\"location_country\"]):\n",
    "                    fill_state = users[\n",
    "                        (users[\"location_country\"] == row[\"location_country\"])\n",
    "                        & (users[\"location_city\"] == row[\"location_city\"])\n",
    "                    ][\"location_state\"].mode()\n",
    "                    if len(fill_state) > 0:\n",
    "                        users.loc[idx, \"location_state\"] = fill_state[0]\n",
    "                        filled_states += 1\n",
    "\n",
    "        # ë‚¨ì€ê±´ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        mode_country = (\n",
    "            users[\"location_country\"].mode()[0]\n",
    "            if not users[\"location_country\"].mode().empty\n",
    "            else \"unknown\"\n",
    "        )\n",
    "        original_country_missing = users[\"location_country\"].isnull().sum()\n",
    "        users[\"location_country\"].fillna(mode_country, inplace=True)\n",
    "\n",
    "        print(f\"** Country: {filled_countries}ê±´ ë³µì›\")\n",
    "        print(\n",
    "            f\"** Country: ìµœë¹ˆê°’({mode_country})ìœ¼ë¡œ {original_country_missing - filled_countries}ê±´ ëŒ€ì²´\"\n",
    "        )\n",
    "        print(f\"** State: {filled_states}ê±´ ë³µì›\")\n",
    "\n",
    "        users = users.drop([\"location\", \"location_list\"], axis=1)\n",
    "\n",
    "        self.cleaning_report[\"location\"] = {\n",
    "            \"filled_countries\": filled_countries,\n",
    "            \"filled_states\": filled_states,\n",
    "            \"mode_country\": mode_country,\n",
    "        }\n",
    "\n",
    "        print(f\"> Location ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return users\n",
    "\n",
    "    # ì±… ë°ì´í„° ì •ì œ\n",
    "    def clean_books(self, books: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        1. year_of_publication: 1900 ë¯¸ë§Œ ë˜ëŠ” 2025 ì´ˆê³¼ â†’ ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        2. language: ê²°ì¸¡ì¹˜ â†’ ISBN ë²ˆí˜¸ ê¸°ë°˜ ì¶”ë¡  â†’ ì¶”ë¡  ì‹¤íŒ¨ ì‹œ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        3. category: ê²°ì¸¡ì¹˜ â†’\n",
    "            1. ë™ì¼ì €ì && ì¶œíŒë…„ë„ ì˜¤ì°¨2 ë‚´ => ê°™ì€ ì¹´í…Œê³ ë¦¬ë¡œ\n",
    "            2. ê·¸ ì¶œíŒì‚¬ì˜ ìµœë¹ˆ ì¹´í…Œê³ ë¦¬ë¡œ\n",
    "            3. ì œëª© ì‚¬ì „ ë§Œë“¤ì–´ì„œ í‚¤ì›Œë“œ ë§µí•‘\n",
    "            4. Sentence Bert ì„ë² ë”©ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ í†µí•©\n",
    "            5. ë‚˜ë¨¸ì§€ëŠ” unknown\n",
    "\n",
    "\n",
    "        4. book_author: ê²°ì¸¡ì¹˜ â†’ 'unknown'\n",
    "        \"\"\"\n",
    "        print(\"> Books ë°ì´í„° ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        books = books.copy()\n",
    "\n",
    "        # 1. Year of Publication ê²°ì¸¡ì¹˜ â†’ ì¤‘ê°„ê°’ìœ¼ë¡œ\n",
    "        reasonable_years = books[\"year_of_publication\"][\n",
    "            (books[\"year_of_publication\"] >= 1900)\n",
    "            & (books[\"year_of_publication\"] <= 2025)\n",
    "        ]\n",
    "        median_year = reasonable_years.median()\n",
    "\n",
    "        outlier_year_count = (\n",
    "            (books[\"year_of_publication\"] < 1900)\n",
    "            | (books[\"year_of_publication\"] > 2025)\n",
    "        ).sum()\n",
    "\n",
    "        books.loc[\n",
    "            (books[\"year_of_publication\"] < 1900)\n",
    "            | (books[\"year_of_publication\"] > 2025),\n",
    "            \"year_of_publication\",\n",
    "        ] = median_year\n",
    "\n",
    "        print(\n",
    "            f\"** ì¶œíŒì—°ë„: ë¹„ì •ìƒ ë²”ìœ„ {outlier_year_count}ê±´ â†’ ì¤‘ê°„ê°’({median_year:.0f})ìœ¼ë¡œ ëŒ€ì²´\"\n",
    "        )\n",
    "\n",
    "        # 2. Language ê²°ì¸¡ì¹˜ â†’ ISBN ë²ˆí˜¸ ê¸°ë°˜ ì¶”ë¡  â†’ ì¶”ë¡  ì‹¤íŒ¨ ì‹œ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        original_missing = books[\"language\"].isnull().sum()\n",
    "        print(f\"** ì›ë³¸ ê²°ì¸¡ì¹˜: {original_missing:,}ê±´\")\n",
    "\n",
    "        missing_mask = books[\"language\"].isnull()\n",
    "        books.loc[missing_mask, \"language\"] = books.loc[missing_mask, \"isbn\"].apply(\n",
    "            self.isbn_parser.infer_language_from_isbn\n",
    "        )\n",
    "\n",
    "        # ISBN ì¶”ë¡  í›„ ë‚¨ì€ ê²°ì¸¡ì¹˜\n",
    "        remaining_missing = books[\"language\"].isnull().sum()\n",
    "        isbn_filled = original_missing - remaining_missing\n",
    "\n",
    "        print(\n",
    "            f\"** ISBNìœ¼ë¡œ ìœ ì¶”í•œ ê°œìˆ˜: {isbn_filled:,}ê±´ ({isbn_filled/original_missing*100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # ë‚¨ì€ ê²°ì¸¡ì¹˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "        if remaining_missing > 0:\n",
    "            mode_language = (\n",
    "                books[\"language\"].mode()[0]\n",
    "                if not books[\"language\"].mode().empty\n",
    "                else \"en\"\n",
    "            )\n",
    "            books[\"language\"].fillna(mode_language, inplace=True)\n",
    "            print(f\"** ìµœë¹ˆê°’({mode_language})ìœ¼ë¡œ ë‚˜ë¨¸ì§€ {remaining_missing:,}ê±´ ëŒ€ì²´\")\n",
    "\n",
    "        # ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸\n",
    "        if isbn_filled > 0:\n",
    "            filled_langs = books.loc[missing_mask, \"language\"].value_counts().head(5)\n",
    "            print(f\"\\n  [ISBNìœ¼ë¡œ ì¶”ë¡ í•œ ì–¸ì–´ ë¶„í¬ í™•ì¸ (Top 5)]\")\n",
    "            for lang, count in filled_langs.items():\n",
    "                print(f\"- {lang}: {count:,}ê±´\")\n",
    "\n",
    "        # 3. Category ì •ì œ\n",
    "        def str2list(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            if x[0] != \"[\":\n",
    "                return x.split(\", \")\n",
    "            return x[1:-1].split(\", \")\n",
    "\n",
    "        books[\"category\"] = books[\"category\"].apply(\n",
    "            lambda x: str2list(x)[0] if not pd.isna(x) else np.nan\n",
    "        )\n",
    "\n",
    "        if self.use_advanced_category:\n",
    "            books, category_report = self.category_handler.fill_category_missing(books)\n",
    "            self.cleaning_report[\"category_advanced\"] = category_report\n",
    "            category_missing = category_report.get(\"final_unknown\", 0)\n",
    "\n",
    "        else:\n",
    "            books[\"category\"].fillna(\"unknown\", inplace=True)\n",
    "            category_missing = (books[\"category\"] == \"unknown\").sum()\n",
    "            print(f\"** Category: ê²°ì¸¡ì¹˜ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        # 4. Book Author ê²°ì¸¡ì¹˜ â†’ unknown\n",
    "        author_missing = books[\"book_author\"].isnull().sum()\n",
    "        books[\"book_author\"].fillna(\"unknown\", inplace=True)\n",
    "        print(f\"** Book Author: ê²°ì¸¡ì¹˜ {author_missing}ê±´ â†’ 'unknown'ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "        self.cleaning_report[\"books\"] = {\n",
    "            \"outlier_year_count\": outlier_year_count,\n",
    "            \"median_year\": median_year,\n",
    "            \"language_missing\": original_missing,\n",
    "            \"category_missing\": category_missing,\n",
    "            \"author_missing\": author_missing,\n",
    "        }\n",
    "\n",
    "        print(f\"> Books ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return books\n",
    "\n",
    "    # í‰ì  ë°ì´í„° ì •ì œ\n",
    "    def clean_ratings(\n",
    "        self, train: pd.DataFrame, test: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
    "        \"\"\"\n",
    "        print(\"> Ratings ë°ì´í„° ì •ì œ ì‹œì‘\")\n",
    "\n",
    "        train = train.copy()\n",
    "        test = test.copy()\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° (user_id + isbn ê¸°ì¤€)\n",
    "        train_duplicates = train.duplicated(subset=[\"user_id\", \"isbn\"]).sum()\n",
    "        if train_duplicates > 0:\n",
    "            train = train.drop_duplicates(subset=[\"user_id\", \"isbn\"], keep=\"last\")\n",
    "            print(f\"** Train: ì¤‘ë³µ ë°ì´í„° {train_duplicates}ê±´ ì œê±°\")\n",
    "\n",
    "        test_duplicates = test.duplicated(subset=[\"user_id\", \"isbn\"]).sum()\n",
    "        if test_duplicates > 0:\n",
    "            test = test.drop_duplicates(subset=[\"user_id\", \"isbn\"], keep=\"last\")\n",
    "            print(f\"** Test: ì¤‘ë³µ ë°ì´í„° {test_duplicates}ê±´ ì œê±°\")\n",
    "\n",
    "        self.cleaning_report[\"ratings\"] = {\n",
    "            \"train_duplicates\": train_duplicates,\n",
    "            \"test_duplicates\": test_duplicates,\n",
    "        }\n",
    "\n",
    "        print(f\"> Ratings ì •ì œ ì™„ë£Œ\\n\")\n",
    "        return train, test\n",
    "\n",
    "    def save_cleaned_data(\n",
    "        self,\n",
    "        users: pd.DataFrame,\n",
    "        books: pd.DataFrame,\n",
    "        train: pd.DataFrame,\n",
    "        test: pd.DataFrame,\n",
    "    ):\n",
    "        print(\">>> ì •ì œëœ ë°ì´í„° ì €ì¥ ì‹œì‘\")\n",
    "\n",
    "        additional_path = (\n",
    "            self.output_path + \"v4/\"\n",
    "        )  # TODO: ë°ì´í„° ë²„ì „ë³„ë¡œ íŒŒì¼ëª… ìˆ˜ì •í•˜ê¸°\n",
    "        os.makedirs(additional_path, exist_ok=True)\n",
    "        print(f\"** ì €ì¥ ê²½ë¡œ: {additional_path}\")\n",
    "\n",
    "        # ì €ì¥ ì‹œ, Location ì»¬ëŸ¼ ì¬ê²°í•©\n",
    "        if all(\n",
    "            col in users.columns\n",
    "            for col in [\"location_country\", \"location_state\", \"location_city\"]\n",
    "        ):\n",
    "\n",
    "            def combine_location(row):\n",
    "                parts = []\n",
    "\n",
    "                # city, state, country\n",
    "                if pd.notna(row[\"location_city\"]) and row[\"location_city\"] != \"unknown\":\n",
    "                    parts.append(str(row[\"location_city\"]))\n",
    "\n",
    "                if (\n",
    "                    pd.notna(row[\"location_state\"])\n",
    "                    and row[\"location_state\"] != \"unknown\"\n",
    "                ):\n",
    "                    parts.append(str(row[\"location_state\"]))\n",
    "\n",
    "                if (\n",
    "                    pd.notna(row[\"location_country\"])\n",
    "                    and row[\"location_country\"] != \"unknown\"\n",
    "                ):\n",
    "                    parts.append(str(row[\"location_country\"]))\n",
    "\n",
    "                # ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "                if not parts:\n",
    "                    return \"unknown, unknown, unknown\"\n",
    "\n",
    "                return \", \".join(parts)\n",
    "\n",
    "            users[\"location\"] = users.apply(combine_location, axis=1)\n",
    "\n",
    "            # íŒŒì‹±ëœ ì»¬ëŸ¼ ì œê±°\n",
    "            users = users.drop(\n",
    "                [\"location_country\", \"location_state\", \"location_city\"], axis=1\n",
    "            )\n",
    "\n",
    "        users.to_csv(additional_path + \"users.csv\", index=False)\n",
    "        books.to_csv(additional_path + \"books.csv\", index=False)\n",
    "        train.to_csv(additional_path + \"train_ratings.csv\", index=False)\n",
    "        test.to_csv(additional_path + \"test_ratings.csv\", index=False)\n",
    "\n",
    "        print(f\"  - {additional_path}users.csv\")\n",
    "        print(f\"  - {additional_path}books.csv\")\n",
    "        print(f\"  - {additional_path}train_ratings.csv\")\n",
    "        print(f\"  - {additional_path}test_ratings.csv\")\n",
    "\n",
    "        print(\"\\n<<< ì €ì¥ ì™„ë£Œ! >>>\\n\")\n",
    "\n",
    "    def generate_cleaning_report(self):\n",
    "        print(\"=\" * 10)\n",
    "        print(\" \" * 20 + \">>> ë°ì´í„° ì •ì œ ê²°ê³¼ >>> \")\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "        print(\"\\n[ì›ë³¸ ë°ì´í„° í¬ê¸°]\")\n",
    "        for key, value in self.cleaning_report[\"original_sizes\"].items():\n",
    "            print(f\"  - {key.capitalize()}: {value:,} rows\")\n",
    "\n",
    "        print(\"\\n[Age ì •ì œ]\")\n",
    "        age_report = self.cleaning_report.get(\"age\", {})\n",
    "        print(f\"- ê²°ì¸¡ì¹˜ ëŒ€ì²´: {age_report.get('original_missing', 0):,}ê±´\")\n",
    "\n",
    "        print(\"\\n[Location ì •ì œ]\")\n",
    "        loc_report = self.cleaning_report.get(\"location\", {})\n",
    "        print(f\"- Country ë³µì›: {loc_report.get('filled_countries', 0)}ê±´\")\n",
    "        print(f\"- State ë³µì›: {loc_report.get('filled_states', 0)}ê±´\")\n",
    "        print(f\"- Country ìµœë¹ˆê°’: {loc_report.get('mode_country', 'N/A')}\")\n",
    "\n",
    "        print(\"\\n[Books ì •ì œ]\")\n",
    "        books_report = self.cleaning_report.get(\"books\", {})\n",
    "        print(f\"- ì¶œíŒì—°ë„ ì´ìƒì¹˜: {books_report.get('outlier_year_count', 0)}ê±´\")\n",
    "        print(f\"- ì‚¬ìš©ëœ ì¤‘ì•™ ì—°ë„: {books_report.get('median_year', 0):.0f}\")\n",
    "        print(f\"- Language ê²°ì¸¡ì¹˜: {books_report.get('language_missing', 0):,}ê±´\")\n",
    "        print(f\"- Category ê²°ì¸¡ì¹˜: {books_report.get('category_missing', 0):,}ê±´\")\n",
    "        print(f\"- Author ê²°ì¸¡ì¹˜: {books_report.get('author_missing', 0)}ê±´\")\n",
    "        if \"category_advanced\" in self.cleaning_report:\n",
    "            cat_adv = self.cleaning_report[\"category_advanced\"]\n",
    "            print(\"\\n[Advanced Category ì²˜ë¦¬ ìƒì„¸]\")\n",
    "            print(f\"- ì›ë³¸ ê²°ì¸¡ì¹˜: {cat_adv.get('original_missing', 0):,}ê±´\")\n",
    "            print(f\"- Step 1 (ì €ì+ì—°ë„): {cat_adv.get('step1_filled', 0):,}ê±´\")\n",
    "            print(f\"- Step 2 (Publisher): {cat_adv.get('step2_filled', 0):,}ê±´\")\n",
    "            print(f\"- Step 3 (í‚¤ì›Œë“œ): {cat_adv.get('step3_filled', 0):,}ê±´\")\n",
    "            print(f\"- Step 4 (í´ëŸ¬ìŠ¤í„°ë§): {cat_adv.get('step4_filled', 0):,}ê±´\")\n",
    "            print(f\"- ìµœì¢… unknown: {cat_adv.get('final_unknown', 0):,}ê±´\")\n",
    "\n",
    "        print(\"\\n[Ratings ì •ì œ]\")\n",
    "        ratings_report = self.cleaning_report.get(\"ratings\", {})\n",
    "        print(f\"- Train ì¤‘ë³µ ì œê±°: {ratings_report.get('train_duplicates', 0)}ê±´\")\n",
    "        print(f\"- Test ì¤‘ë³µ ì œê±°: {ratings_report.get('test_duplicates', 0)}ê±´\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 10)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"ì „ì²´ ì‹¤í–‰\"\"\"\n",
    "\n",
    "        # 1. ë°ì´í„° ë¡œë“œ\n",
    "        users, books, train, test = self.load_data()\n",
    "\n",
    "        # 2. Users ì •ì œ\n",
    "        users = self.clean_age(users)\n",
    "        users = self.clean_location(users)\n",
    "\n",
    "        # 3. Books ì •ì œ\n",
    "        books = self.clean_books(books)\n",
    "\n",
    "        # 4. Ratings ì •ì œ\n",
    "        train, test = self.clean_ratings(train, test)\n",
    "\n",
    "        # 5. ì €ì¥\n",
    "        self.save_cleaned_data(users, books, train, test)\n",
    "\n",
    "        # 6. ë³´ê³ ì„œ ì¶œë ¥\n",
    "        self.generate_cleaning_report()\n",
    "\n",
    "        return users, books, train, test\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"/data/ephemeral/home/sojin/data/\"  # TODO: ì›ë³¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "\n",
    "    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    pipeline = DataCleaningPipeline(DATA_PATH)\n",
    "    users_cleaned, books_cleaned, train_cleaned, test_cleaned = pipeline.run()\n",
    "\n",
    "    print(\"\\nğŸ‰ ğŸ‰ ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

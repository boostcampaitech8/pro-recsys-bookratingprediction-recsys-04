import os
from tqdm import tqdm
import torch
from src.loss import loss as loss_module
import torch.optim as optimizer_module
import torch.optim.lr_scheduler as scheduler_module
from scipy.sparse import csr_matrix, vstack

METRIC_NAMES = {
    "RMSELoss": "RMSE",
    "MSELoss": "MSE",
    "MAELoss": "MAE",
    "VAELoss": "VAE",
}


def train(args, model, dataloader, logger, setting):
    if args.wandb:
        import wandb

    minimum_loss = None

    loss_fn = getattr(loss_module, args.loss)().to(args.device)
    args.metrics = sorted(
        [metric for metric in set(args.metrics) if metric != args.loss]
    )

    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = getattr(optimizer_module, args.optimizer.type)(
        trainable_params, **args.optimizer.args)
    
    # MFì¸ê²½ìš° ëª¨ë¸ ì˜µí‹°ë§ˆì´ì € ìˆ˜ì •
    if args.model == 'MF':
        ########## ì¶”ê°€ ì‹¤í—˜ ##############
        # 1. Config ë”•ì…”ë„ˆë¦¬ë¥¼ ë³µì‚¬í•©ë‹ˆë‹¤. (ì›ë³¸ args í›¼ì† ë°©ì§€)
        optimizer_args = args.optimizer.args.copy()
        
        # 2. 'weight_decay' ê°’ì„ ë½‘ì•„ëƒ…ë‹ˆë‹¤. (ë”•ì…”ë„ˆë¦¬ì—ì„œëŠ” ì‚­ì œë¨)
        # ë§Œì•½ configì— weight_decayê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0.0ì„ ì”ë‹ˆë‹¤.
        weight_decay = optimizer_args.pop('weight_decay', 0.0)
        
        decay_params = []
        no_decay_params = []

        # 3. ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì´ë¦„ì„ í™•ì¸í•˜ë©° ê·¸ë£¹ ë‚˜ëˆ„ê¸°
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
                
            # [í•µì‹¬] ì´ë¦„ì´ .biasë¡œ ëë‚˜ë©´ (Global Bias, Layer Bias) -> ê·œì œ ì œì™¸
            # ì£¼ì˜: User BiasëŠ” ì„ë² ë”©ì´ë¯€ë¡œ ì´ë¦„ì´ 'weight'ë¼ ì—¬ê¸° ê±¸ë¦¬ì§€ ì•ŠìŒ (ê·œì œ ì ìš©ë¨ O)
            if name.endswith('.bias'):
                no_decay_params.append(param)
            else:
                decay_params.append(param)

        # 4. ê·¸ë£¹ë³„ ì„¤ì • ìƒì„±
        param_groups = [
            # ê·¸ë£¹ A: Configì— ì íŒ weight_decay (1e-4) ì ìš©
            {'params': decay_params, 'weight_decay': weight_decay},
            
            # ê·¸ë£¹ B: Weight Decay 0.0 ê°•ì œ ì ìš©
            {'params': no_decay_params, 'weight_decay': 0.0}
        ]

        # 5. ì˜µí‹°ë§ˆì´ì € ìƒì„±
        # param_groups: ìš°ë¦¬ê°€ ë‚˜ëˆˆ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸
        # **optimizer_args: weight_decayê°€ ë¹ ì§„ ë‚˜ë¨¸ì§€ ì„¤ì •ë“¤ (lr=1e-4, amsgrad=False)
        optimizer = getattr(optimizer_module, args.optimizer.type)(
            param_groups, **optimizer_args
        )
        ############ ì¶”ê°€ ì‹¤í—˜ ############

    if args.lr_scheduler.use:
        args.lr_scheduler.args = {
            k: v
            for k, v in args.lr_scheduler.args.items()
            if k
            in getattr(
                scheduler_module, args.lr_scheduler.type
            ).__init__.__code__.co_varnames
        }
        lr_scheduler = getattr(scheduler_module, args.lr_scheduler.type)(
            optimizer, **args.lr_scheduler.args
        )
    else:
        lr_scheduler = None

    # [ìˆ˜ì •] Best Model ì •ë³´ ì €ì¥ìš© ë³€ìˆ˜
    best_summary = "Best model logic was not triggered."

    # [ì¶”ê°€] Early Stopping ë³€ìˆ˜ ì´ˆê¸°í™”
    # configì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 10ìœ¼ë¡œ ì„¤ì • (ì•ˆì „ì¥ì¹˜)
    early_stopping_patience = getattr(args.train, "early_stopping_patience", 10)
    patience_check = 0

    print(f"[*] Early Stopping Settings: Patience = {early_stopping_patience}")

    for epoch in range(args.train.epochs):
        model.train()
        total_loss, train_len = 0, len(dataloader["train_dataloader"])

        for data in tqdm(
            dataloader["train_dataloader"],
            desc=f"[Epoch {epoch+1:02d}/{args.train.epochs:02d}]",
        ):
            if args.model_args[args.model].datatype == "image":
                x, y = [
                    data["user_book_vector"].to(args.device),
                    data["img_vector"].to(args.device),
                ], data["rating"].to(args.device)
            elif args.model_args[args.model].datatype == "text":
                x, y = [
                    data["user_book_vector"].to(args.device),
                    data["user_summary_vector"].to(args.device),
                    data["book_summary_vector"].to(args.device),
                ], data["rating"].to(args.device)
            elif args.model_args[args.model].datatype == "sparse":
                x = y = data.to(args.device)
            else:
                x, y = data[0].to(args.device), data[1].to(args.device)

            if args.model_args[args.model].datatype == "sparse":
                y_hat, mu, logvar = model(x)
                loss = loss_fn(y_hat * torch.sign(y), y.float(), mu, logvar)

            else:
                y_hat = model(x)
                loss = loss_fn(y_hat, y.float())

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=5)
            optimizer.step()
            total_loss += loss.item()

        if args.lr_scheduler.use and args.lr_scheduler.type != "ReduceLROnPlateau":
            lr_scheduler.step()

        msg = ""
        train_loss = total_loss / train_len
        msg += f"\tTrain Loss ({METRIC_NAMES[args.loss]}): {train_loss:.3f}"

        if args.dataset.valid_ratio != 0:  # valid ë°ì´í„°ê°€ ì¡´ì¬í•  ê²½ìš°
            valid_loss = valid(args, model, dataloader["valid_dataloader"], loss_fn)
            msg += f"\n\tValid Loss ({METRIC_NAMES[args.loss]}): {valid_loss:.3f}"

            # ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì—¬ê¸°ì„œ valid_lossë¥¼ ë³´ê³  step
            if args.lr_scheduler.use and args.lr_scheduler.type == "ReduceLROnPlateau":
                lr_scheduler.step(valid_loss)
                current_lr = optimizer.param_groups[0]["lr"]
                print(f"\t>> Current LR after scheduler: {current_lr:.6f}")
            valid_metrics = dict()
            for metric in args.metrics:
                metric_fn = getattr(loss_module, metric)().to(args.device)
                valid_metric = valid(
                    args, model, dataloader["valid_dataloader"], metric_fn
                )
                valid_metrics[f"Valid {METRIC_NAMES[metric]}"] = valid_metric
            for metric, value in valid_metrics.items():
                msg += f" | {metric}: {value:.3f}"
            print(msg)
            logger.log(
                epoch=epoch + 1,
                train_loss=train_loss,
                valid_loss=valid_loss,
                valid_metrics=valid_metrics,
            )
            if args.wandb:
                wandb.log(
                    {
                        f"Train {METRIC_NAMES[args.loss]}": train_loss,
                        f"Valid {METRIC_NAMES[args.loss]}": valid_loss,
                        **valid_metrics,
                    }
                )
        else:  # valid ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°
            print(msg)
            logger.log(epoch=epoch + 1, train_loss=train_loss)
            if args.wandb:
                wandb.log({f"Train {METRIC_NAMES[args.loss]}": train_loss})

        # [ìˆ˜ì •] ëª¨ë¸ ì €ì¥ ë° Early Stopping ë¡œì§ ì ìš©
        if args.train.save_best_model:
            best_loss = valid_loss if args.dataset.valid_ratio != 0 else train_loss

            # ì„±ëŠ¥ ê°±ì‹  ì„±ê³µ (Best Model)
            if minimum_loss is None or minimum_loss > best_loss:
                minimum_loss = best_loss
                patience_check = 0  # ì¹´ìš´íŠ¸ ì´ˆê¸°í™”

                os.makedirs(args.train.ckpt_dir, exist_ok=True)
                torch.save(
                    model.state_dict(),
                    f"{args.train.ckpt_dir}/{setting.save_time}_{args.model}_best.pt",
                )
                best_summary = f"[Epoch {epoch+1:02d}] {msg.strip()}"

            # ì„±ëŠ¥ ê°±ì‹  ì‹¤íŒ¨ (Early Stopping ì¹´ìš´íŠ¸ ì¦ê°€)
            else:
                patience_check += 1
                if patience_check >= early_stopping_patience:
                    print(
                        f"\n[Early Stopping] Epoch {epoch+1}ì—ì„œ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤. (Validation Lossê°€ {early_stopping_patience}íšŒ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•ŠìŒ)"
                    )
                    break
        else:
            os.makedirs(args.train.ckpt_dir, exist_ok=True)
            torch.save(
                model.state_dict(),
                f"{args.train.ckpt_dir}/{setting.save_time}_{args.model}_e{epoch:02}.pt",
            )

    print(f"\n{'='*20} TRAINING SUMMARY {'='*20}")
    print("ğŸ† Best Model Performance:")
    print(best_summary)
    print(f"{'='*58}\n")

    logger.close()

    return model


def valid(args, model, dataloader, loss_fn):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        if args.model_args[args.model].datatype == "sparse":
            for train_data, valid_data in dataloader:
                train_data, valid_data = train_data.to(args.device), valid_data.to(
                    args.device
                )
                y_hat, mu, logvar = model(train_data)
                if isinstance(loss_fn, loss_module.VAELoss):
                    loss = loss_fn(
                        y_hat * torch.sign(valid_data), valid_data.float(), mu, logvar
                    )
                else:
                    loss = loss_fn(y_hat * torch.sign(valid_data), valid_data.float())
                total_loss += loss.item()

        else:
            for data in dataloader:
                if args.model_args[args.model].datatype == "image":
                    x, y = [
                        data["user_book_vector"].to(args.device),
                        data["img_vector"].to(args.device),
                    ], data["rating"].to(args.device)
                elif args.model_args[args.model].datatype == "text":
                    x, y = [
                        data["user_book_vector"].to(args.device),
                        data["user_summary_vector"].to(args.device),
                        data["book_summary_vector"].to(args.device),
                    ], data["rating"].to(args.device)
                else:
                    x, y = data[0].to(args.device), data[1].to(args.device)
                y_hat = model(x)
                loss = loss_fn(y_hat, y.float())
                total_loss += loss.item()

    return total_loss / len(dataloader)


def test(args, model, data, setting, checkpoint=None):
    predicts = list()
    if checkpoint:
        model.load_state_dict(torch.load(checkpoint, weights_only=True))
    else:
        if args.train.save_best_model:
            model_path = (
                f"{args.train.ckpt_dir}/{setting.save_time}_{args.model}_best.pt"
            )
        else:
            # bestê°€ ì•„ë‹ ê²½ìš° ë§ˆì§€ë§‰ ì—í­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ í•¨
            model_path = f"{args.train.save_dir.checkpoint}/{setting.save_time}_{args.model}_e{args.train.epochs-1:02d}.pt"
        model.load_state_dict(torch.load(model_path, weights_only=True))

    model.eval()

    with torch.no_grad():
        if args.model_args[args.model].datatype == "sparse":
            predicts_list = []

            for train_data, test_data in data["test_dataloader"]:
                x = train_data.to(args.device)
                test_data = test_data.to(args.device)
                y_hat, _, _ = model(x)
                y_hat = y_hat * torch.sign(test_data)
                y_hat = y_hat.cpu().detach().numpy()
                predicts_list.append(csr_matrix(y_hat))

            predicts_csr = vstack(predicts_list)

            rows = data["test"]["user_id"].values
            cols = data["test"]["isbn"].values

            predicts = predicts_csr[rows, cols].A1.tolist()

        else:
            for data in data["test_dataloader"]:
                if args.model_args[args.model].datatype == "image":
                    x = [
                        data["user_book_vector"].to(args.device),
                        data["img_vector"].to(args.device),
                    ]
                elif args.model_args[args.model].datatype == "text":
                    x = [
                        data["user_book_vector"].to(args.device),
                        data["user_summary_vector"].to(args.device),
                        data["book_summary_vector"].to(args.device),
                    ]
                else:
                    x = data[0].to(args.device)
                y_hat = model(x)
                predicts.extend(y_hat.tolist())

    return predicts

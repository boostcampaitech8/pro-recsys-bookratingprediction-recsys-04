import numpy as np
import pandas as pd
import regex
import torch
from torch.utils.data import TensorDataset, DataLoader
from .basic_data import basic_data_split


def str2list(x: str) -> list:
    """ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    return x[1:-1].split(", ")


def split_location(x: str) -> list:
    """
    Parameters
    ----------
    x : str
        location ë°ì´í„°

    Returns
    -------
    res : list
        location ë°ì´í„°ë¥¼ ë‚˜ëˆˆ ë’¤, ì •ì œí•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ìˆœì„œëŠ” country, state, city, ... ì…ë‹ˆë‹¤.
    """
    res = x.split(",")
    res = [i.strip().lower() for i in res]
    res = [regex.sub(r"[^a-zA-Z/ ]", "", i) for i in res]  # remove special characters
    res = [i if i not in ["n/a", ""] else np.nan for i in res]  # change 'n/a' into NaN
    res.reverse()  # reverse the list to get country, state, city, ... order

    for i in range(len(res) - 1, 0, -1):
        if (res[i] in res[:i]) and (
            not pd.isna(res[i])
        ):  # remove duplicated values if not NaN
            res.pop(i)

    return res


def process_context_data(users, books):
    """
    users.csvì™€ books.csvë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    (location ë¬¸ìì—´ ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ country/state/cityë§Œ ì‚¬ìš©)
    """

    users_ = users.copy()
    books_ = books.copy()

    # -----------------------------
    # ğŸ“Œ BOOKS ì „ì²˜ë¦¬
    # -----------------------------
    books_["category"] = books_["category"].apply(
        lambda x: str2list(x)[0] if not pd.isna(x) else np.nan
    )
    books_["language"] = books_["language"].fillna(books_["language"].mode()[0])
    books_["publication_range"] = books_["year_of_publication"].apply(
        lambda x: x // 10 * 10
    )

    # -----------------------------
    # ğŸ“Œ USERS ì „ì²˜ë¦¬
    # -----------------------------
    users_["age"] = users_["age"].fillna(users_["age"].mode()[0])
    users_["age_range"] = users_["age"].apply(lambda x: x // 10 * 10)

    # ğŸ“Œ location_country/state/cityëŠ” ì´ë¯¸ users.csvì— ì¡´ì¬í•¨
    # â†’ ì—¬ê¸°ì„œëŠ” ê²°ì¸¡ì¹˜ë§Œ ë‹¨ìˆœ ì²˜ë¦¬
    if "location_country" not in users_.columns:
        users_["location_country"] = "unknown"

    if "location_state" not in users_.columns:
        users_["location_state"] = "unknown"

    if "location_city" not in users_.columns:
        users_["location_city"] = "unknown"

    users_["location_country"] = users_["location_country"].fillna("unknown")
    users_["location_state"] = users_["location_state"].fillna("unknown")
    users_["location_city"] = users_["location_city"].fillna("unknown")

    # ğŸ“Œ ê¸°ì¡´ location ë¬¸ìì—´ ê¸°ë°˜ íŒŒì‹±ì€ ì œê±°
    # (users_["location_list"], split_location ë“± ì‚­ì œ)

    return users_, books_


# def process_context_data(users, books):
#     """
#     Parameters
#     ----------
#     users : pd.DataFrame
#         users.csvë¥¼ ì¸ë±ì‹±í•œ ë°ì´í„°
#     books : pd.DataFrame
#         books.csvë¥¼ ì¸ë±ì‹±í•œ ë°ì´í„°
#     ratings1 : pd.DataFrame
#         train ë°ì´í„°ì˜ rating
#     ratings2 : pd.DataFrame
#         test ë°ì´í„°ì˜ rating

#     Returns
#     -------
#     label_to_idx : dict
#         ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•œ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
#     idx_to_label : dict
#         ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ì›ë˜ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
#     train_df : pd.DataFrame
#         train ë°ì´í„°
#     test_df : pd.DataFrame
#         test ë°ì´í„°
#     """

#     users_ = users.copy()
#     books_ = books.copy()

#     # ë°ì´í„° ì „ì²˜ë¦¬ (ì „ì²˜ë¦¬ëŠ” ê°ìì˜ ìƒí™©ì— ë§ê²Œ ì§„í–‰í•´ì£¼ì„¸ìš”!)
#     books_["category"] = books_["category"].apply(
#         lambda x: str2list(x)[0] if not pd.isna(x) else np.nan
#     )
#     books_["language"] = books_["language"].fillna(books_["language"].mode()[0])
#     books_["publication_range"] = books_["year_of_publication"].apply(
#         lambda x: x // 10 * 10
#     )  # 1990ë…„ëŒ€, 2000ë…„ëŒ€, 2010ë…„ëŒ€, ...

#     users_["age"] = users_["age"].fillna(users_["age"].mode()[0])
#     users_["age_range"] = users_["age"].apply(
#         lambda x: x // 10 * 10
#     )  # 10ëŒ€, 20ëŒ€, 30ëŒ€, ...

#     users_["location_list"] = users_["location"].apply(lambda x: split_location(x))
#     users_["location_country"] = users_["location_list"].apply(lambda x: x[0])
#     users_["location_state"] = users_["location_list"].apply(
#         lambda x: x[1] if len(x) > 1 else np.nan
#     )
#     users_["location_city"] = users_["location_list"].apply(
#         lambda x: x[2] if len(x) > 2 else np.nan
#     )
#     for idx, row in users_.iterrows():
#         if (not pd.isna(row["location_state"])) and pd.isna(row["location_country"]):
#             fill_country = users_[users_["location_state"] == row["location_state"]][
#                 "location_country"
#             ].mode()
#             fill_country = fill_country[0] if len(fill_country) > 0 else np.nan
#             users_.loc[idx, "location_country"] = fill_country
#         elif (not pd.isna(row["location_city"])) and pd.isna(row["location_state"]):
#             if not pd.isna(row["location_country"]):
#                 fill_state = users_[
#                     (users_["location_country"] == row["location_country"])
#                     & (users_["location_city"] == row["location_city"])
#                 ]["location_state"].mode()
#                 fill_state = fill_state[0] if len(fill_state) > 0 else np.nan
#                 users_.loc[idx, "location_state"] = fill_state
#             else:
#                 fill_state = users_[users_["location_city"] == row["location_city"]][
#                     "location_state"
#                 ].mode()
#                 fill_state = fill_state[0] if len(fill_state) > 0 else np.nan
#                 fill_country = users_[users_["location_city"] == row["location_city"]][
#                     "location_country"
#                 ].mode()
#                 fill_country = fill_country[0] if len(fill_country) > 0 else np.nan
#                 users_.loc[idx, "location_country"] = fill_country
#                 users_.loc[idx, "location_state"] = fill_state

#     users_ = users_.drop(["location"], axis=1)

#     return users_, books_


def context_data_load(args):
    """
    Parameters
    ----------
    args.dataset.data_path : str
        ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆëŠ” parser

    Returns
    -------
    data : dict
        í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë‹´ê¸´ ì‚¬ì „ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    ######################## DATA LOAD
    users = pd.read_csv(args.dataset.data_path + "users.csv")
    books = pd.read_csv(args.dataset.data_path + "books.csv")
    train = pd.read_csv(args.dataset.data_path + "train_ratings.csv")
    test = pd.read_csv(args.dataset.data_path + "test_ratings.csv")
    sub = pd.read_csv(args.dataset.data_path + "sample_submission.csv")

    users_, books_ = process_context_data(users, books)

    # ìœ ì € ë° ì±… ì •ë³´ë¥¼ í•©ì³ì„œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
    # ì‚¬ìš©í•  ì»¬ëŸ¼ì„ user_featuresì™€ book_featuresì— ì •ì˜í•©ë‹ˆë‹¤. (ë‹¨, ëª¨ë‘ ë²”ì£¼í˜• ë°ì´í„°ë¡œ ê°€ì •)
    # ë² ì´ìŠ¤ë¼ì¸ì—ì„œëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
    # NCFë¥¼ ì‚¬ìš©í•  ê²½ìš°, idx 0, 1ì€ ê°ê° user_id, isbnì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    user_features = [
        "user_id",
        "age_range",
        "location_country",
        "location_state",
        "location_city",
    ]
    book_features = [
        "isbn",
        "book_title",
        "book_author",
        "publisher",
        "language",
        "category",
        "publication_range",
    ]
    sparse_cols = (
        ["user_id", "isbn"]
        + list(set(user_features + book_features) - {"user_id", "isbn"})
        if args.model == "NCF"
        else user_features + book_features
    )

    # ì„ íƒí•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë°ì´í„° ì¡°ì¸
    train_df = train.merge(users_, on="user_id", how="left").merge(
        books_, on="isbn", how="left"
    )[sparse_cols + ["rating"]]
    test_df = test.merge(users_, on="user_id", how="left").merge(
        books_, on="isbn", how="left"
    )[sparse_cols]
    all_df = pd.concat([train_df, test_df], axis=0)

    # feature_colsì˜ ë°ì´í„°ë§Œ ë¼ë²¨ ì¸ì½”ë”©í•˜ê³  ì¸ë±ìŠ¤ ì •ë³´ë¥¼ ì €ì¥
    label2idx, idx2label = {}, {}
    for col in sparse_cols:
        # 1. ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìœ ë‹ˆí¬ ë¼ë²¨ ì¶”ì¶œ ë° ì‚¬ì „ ìƒì„±
        all_df[col] = all_df[col].fillna("unknown")
        unique_labels = all_df[col].astype("category").cat.categories
        label2idx[col] = {label: idx for idx, label in enumerate(unique_labels)}
        idx2label[col] = {idx: label for idx, label in enumerate(unique_labels)}

        # 2. [ìˆ˜ì •ë¨] ìƒì„±ëœ ì‚¬ì „ì„ ì´ìš©í•´ train/test ì¼ê´€ì„± ìˆê²Œ ë§¤í•‘
        # ì£¼ì˜: merge ê³¼ì •ì—ì„œ ìƒê¸¸ ìˆ˜ ìˆëŠ” NaNì„ ìœ„í•´ ì—¬ê¸°ì„œë„ fillnaë¥¼ í•´ì¤ë‹ˆë‹¤.
        train_df[col] = train_df[col].fillna("unknown").map(label2idx[col])
        test_df[col] = test_df[col].fillna("unknown").map(label2idx[col])

    # field_dims ê³„ì‚° ì‹œ rating ì»¬ëŸ¼ ì œì™¸í•˜ê³  ì •í™•íˆ ê³„ì‚°
    # (train_df.columnsì—ëŠ” ratingì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜)
    field_dims = [len(label2idx[col]) for col in sparse_cols]

    data = {
        "train": train_df,
        "test": test_df,
        "field_names": sparse_cols,
        "field_dims": field_dims,
        "label2idx": label2idx,
        "idx2label": idx2label,
        "sub": sub,
    }

    return data


def context_data_split(args, data):
    """data ë‚´ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¡œ ë‚˜ëˆ„ì–´ ì¶”ê°€í•œ í›„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return basic_data_split(args, data)


def context_data_loader(args, data):
    """
    Parameters
    ----------
    args.dataloader.batch_size : int
        ë°ì´í„° batchì— ì‚¬ìš©í•  ë°ì´í„° ì‚¬ì´ì¦ˆ
    args.dataloader.shuffle : bool
        data shuffle ì—¬ë¶€
    args.dataloader.num_workers: int
        dataloaderì—ì„œ ì‚¬ìš©í•  ë©€í‹°í”„ë¡œì„¸ì„œ ìˆ˜
    args.dataset.valid_ratio : float
        Train/Valid split ë¹„ìœ¨ë¡œ, 0ì¼ ê²½ìš°ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    data : dict
        context_data_load í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ë°ì´í„°

    Returns
    -------
    data : dict
        DataLoaderê°€ ì¶”ê°€ëœ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    train_dataset = TensorDataset(
        torch.LongTensor(data["X_train"].values),
        torch.LongTensor(data["y_train"].values),
    )
    valid_dataset = (
        TensorDataset(
            torch.LongTensor(data["X_valid"].values),
            torch.LongTensor(data["y_valid"].values),
        )
        if args.dataset.valid_ratio != 0
        else None
    )
    test_dataset = TensorDataset(torch.LongTensor(data["test"].values))

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.dataloader.batch_size,
        shuffle=args.dataloader.shuffle,
        num_workers=args.dataloader.num_workers,
    )
    valid_dataloader = (
        DataLoader(
            valid_dataset,
            batch_size=args.dataloader.batch_size,
            shuffle=False,
            num_workers=args.dataloader.num_workers,
        )
        if args.dataset.valid_ratio != 0
        else None
    )
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=args.dataloader.batch_size,
        shuffle=False,
        num_workers=args.dataloader.num_workers,
    )

    data["train_dataloader"], data["valid_dataloader"], data["test_dataloader"] = (
        train_dataloader,
        valid_dataloader,
        test_dataloader,
    )

    return data
